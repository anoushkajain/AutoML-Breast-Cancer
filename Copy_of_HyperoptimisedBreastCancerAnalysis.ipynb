{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of HyperoptimisedBreastCancerAnalysis.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "XuXNsTnkfp_V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.utils import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_NwoMOj7gCDh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "txA-tupBulBh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv(\"BC_Data1.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wJ0zdofVvV5P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = pd.DataFrame(data=df1,columns= ['Age', 'Tumor Size','Nodes','KI67','Basal-like Score','Luminal A Score','Luminal B Score', 'HER2-enriched Score',\n",
        "                                    'Normal Score','ESR1 Score','ERBB2 Score','PGR Score','Proliferation Score','Luminal Score',\n",
        "                                    'ACTR3B','ANLN','BAG1','BCL2','BLVRA','CCNE1','CDC6','CDH3','CXXC5', 'EGFR' ,'ERBB2', 'ESR1','EXO1', 'FGFR4',\n",
        "                                    'FOXA1', 'FOXC1', 'GRB7', 'KRT14', 'KRT17','KRT5', 'MAPT','MDM2','MIA','MLPH','MMP11', 'MYBL2', 'MYC', 'NAT1',\n",
        "                                    'ORC6L', 'PGR', 'PHGDH', 'SFRP1' , 'SLC39A6', 'UBE2T' ,'CDC20' ,'MKI67', 'RRM2','TYMS', 'UBE2C', 'CENPF', 'GPR160',\n",
        "                                    'KIF2C', 'MELK', 'TMEM45B', 'BIRC5', 'CCNB1','CDCA1','CEP55', 'KNTC2','PTTG1'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uQtYxewGvhxl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X['KI67'] = X['KI67'].fillna(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nT0LnXlWvmvp",
        "colab_type": "code",
        "outputId": "ca46ed30-7dea-49d2-c07a-e6cb07105361",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1190
        }
      },
      "cell_type": "code",
      "source": [
        "X.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 814 entries, 0 to 813\n",
            "Data columns (total 64 columns):\n",
            "Age                    814 non-null float64\n",
            "Tumor Size             814 non-null float64\n",
            "Nodes                  814 non-null int64\n",
            "KI67                   814 non-null float64\n",
            "Basal-like Score       814 non-null float64\n",
            "Luminal A Score        814 non-null float64\n",
            "Luminal B Score        814 non-null float64\n",
            "HER2-enriched Score    814 non-null float64\n",
            "Normal Score           814 non-null float64\n",
            "ESR1 Score             814 non-null float64\n",
            "ERBB2 Score            814 non-null float64\n",
            "PGR Score              814 non-null float64\n",
            "Proliferation Score    814 non-null float64\n",
            "Luminal Score          814 non-null float64\n",
            "ACTR3B                 814 non-null float64\n",
            "ANLN                   814 non-null float64\n",
            "BAG1                   814 non-null float64\n",
            "BCL2                   814 non-null float64\n",
            "BLVRA                  814 non-null float64\n",
            "CCNE1                  814 non-null float64\n",
            "CDC6                   814 non-null float64\n",
            "CDH3                   814 non-null float64\n",
            "CXXC5                  814 non-null float64\n",
            "EGFR                   814 non-null float64\n",
            "ERBB2                  814 non-null float64\n",
            "ESR1                   814 non-null float64\n",
            "EXO1                   814 non-null float64\n",
            "FGFR4                  814 non-null float64\n",
            "FOXA1                  814 non-null float64\n",
            "FOXC1                  814 non-null float64\n",
            "GRB7                   814 non-null float64\n",
            "KRT14                  814 non-null float64\n",
            "KRT17                  814 non-null float64\n",
            "KRT5                   814 non-null float64\n",
            "MAPT                   814 non-null float64\n",
            "MDM2                   814 non-null float64\n",
            "MIA                    814 non-null float64\n",
            "MLPH                   814 non-null float64\n",
            "MMP11                  814 non-null float64\n",
            "MYBL2                  814 non-null float64\n",
            "MYC                    814 non-null float64\n",
            "NAT1                   814 non-null float64\n",
            "ORC6L                  814 non-null float64\n",
            "PGR                    814 non-null float64\n",
            "PHGDH                  814 non-null float64\n",
            "SFRP1                  814 non-null float64\n",
            "SLC39A6                814 non-null float64\n",
            "UBE2T                  814 non-null float64\n",
            "CDC20                  814 non-null float64\n",
            "MKI67                  814 non-null float64\n",
            "RRM2                   814 non-null float64\n",
            "TYMS                   814 non-null float64\n",
            "UBE2C                  814 non-null float64\n",
            "CENPF                  814 non-null float64\n",
            "GPR160                 814 non-null float64\n",
            "KIF2C                  814 non-null float64\n",
            "MELK                   814 non-null float64\n",
            "TMEM45B                814 non-null float64\n",
            "BIRC5                  814 non-null float64\n",
            "CCNB1                  814 non-null float64\n",
            "CDCA1                  814 non-null float64\n",
            "CEP55                  814 non-null float64\n",
            "KNTC2                  814 non-null float64\n",
            "PTTG1                  814 non-null float64\n",
            "dtypes: float64(63), int64(1)\n",
            "memory usage: 407.1 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GX03g4DdvrgS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y = pd.DataFrame(data=df1,columns= ['Subtype Prediction'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K2wb76sH0LLW",
        "colab_type": "code",
        "outputId": "fae5bd9e-3311-4709-dd3c-df2949853d79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "y.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Subtype Prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Luminal A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Normal_Like</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Luminal A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HER2-enriched</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>HER2-enriched</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Subtype Prediction\n",
              "0          Luminal A\n",
              "1        Normal_Like\n",
              "2          Luminal A\n",
              "3      HER2-enriched\n",
              "4      HER2-enriched"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "id": "X9TcSm1AA_kk",
        "colab_type": "code",
        "outputId": "8098b66d-2f80-4931-f8d0-46887aa19e1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "X = np.array(X).astype(np.float64)\n",
        "print(X.shape)\n",
        "y = np.array(y)\n",
        "one = OneHotEncoder() #initiate class\n",
        "y = one.fit_transform(y)\n",
        "print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(814, 64)\n",
            "(814, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w74CL4TSg9RW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1ZvsT9P999Wr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_qVoM-rfjsX_",
        "colab_type": "code",
        "outputId": "d8c88d1b-d75d-454c-b040-ea41a82005fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(f'train shape:{X_train.shape, y_train.shape}, Test shape:{X_test.shape, y_test.shape}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train shape:((651, 64), (651, 5)), Test shape:((163, 64), (163, 5))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E4GbfMowg_Uu",
        "colab_type": "code",
        "outputId": "3fad186a-07dc-4c47-d565-cdd2d4cce361",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(100, input_dim=64, kernel_initializer='normal', activation='relu'))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 100)               6500      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 5)                 505       \n",
            "=================================================================\n",
            "Total params: 17,105\n",
            "Trainable params: 17,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PCn4lhhGhTzX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0ohOZRCdCyMk",
        "colab_type": "code",
        "outputId": "faff3b8f-dfa8-41aa-e7a4-aa8ccae87816",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1107
        }
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, epochs=20, batch_size=50,  verbose=1, validation_split=0.2)\n",
        "validation_loss = history.history['val_loss'][-1]\n",
        "validation_accuracy = history.history['val_acc'][-1]\n",
        "test_score = model.evaluate(x = X_test, y= y_test)\n",
        "\n",
        "print(f'Validation Loss:{validation_loss}, Validation Accuracy:{validation_accuracy}, Test Accuracy:{test_score[1]} with epochs:{20}')\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "#plt.savefig(os.path.joins(directory, 'Training_curve.png'))\n",
        "plt.show()\n",
        "plt.close()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 520 samples, validate on 131 samples\n",
            "Epoch 1/20\n",
            "520/520 [==============================] - 0s 479us/sample - loss: 1.2574 - acc: 0.5058 - val_loss: 1.0106 - val_acc: 0.6183\n",
            "Epoch 2/20\n",
            "520/520 [==============================] - 0s 116us/sample - loss: 0.8217 - acc: 0.7327 - val_loss: 0.6617 - val_acc: 0.8092\n",
            "Epoch 3/20\n",
            "520/520 [==============================] - 0s 113us/sample - loss: 0.5924 - acc: 0.7865 - val_loss: 0.4686 - val_acc: 0.8473\n",
            "Epoch 4/20\n",
            "520/520 [==============================] - 0s 116us/sample - loss: 0.4298 - acc: 0.8596 - val_loss: 0.3752 - val_acc: 0.8550\n",
            "Epoch 5/20\n",
            "520/520 [==============================] - 0s 128us/sample - loss: 0.3554 - acc: 0.8808 - val_loss: 0.3567 - val_acc: 0.9008\n",
            "Epoch 6/20\n",
            "520/520 [==============================] - 0s 118us/sample - loss: 0.3449 - acc: 0.8558 - val_loss: 0.3296 - val_acc: 0.8779\n",
            "Epoch 7/20\n",
            "520/520 [==============================] - 0s 119us/sample - loss: 0.2863 - acc: 0.8923 - val_loss: 0.2428 - val_acc: 0.9313\n",
            "Epoch 8/20\n",
            "520/520 [==============================] - 0s 118us/sample - loss: 0.2434 - acc: 0.9173 - val_loss: 0.2336 - val_acc: 0.9008\n",
            "Epoch 9/20\n",
            "520/520 [==============================] - 0s 123us/sample - loss: 0.2587 - acc: 0.9115 - val_loss: 0.2106 - val_acc: 0.9237\n",
            "Epoch 10/20\n",
            "520/520 [==============================] - 0s 118us/sample - loss: 0.2008 - acc: 0.9327 - val_loss: 0.2404 - val_acc: 0.8931\n",
            "Epoch 11/20\n",
            "520/520 [==============================] - 0s 112us/sample - loss: 0.1934 - acc: 0.9327 - val_loss: 0.1850 - val_acc: 0.9313\n",
            "Epoch 12/20\n",
            "520/520 [==============================] - 0s 120us/sample - loss: 0.1800 - acc: 0.9538 - val_loss: 0.2051 - val_acc: 0.9160\n",
            "Epoch 13/20\n",
            "520/520 [==============================] - 0s 115us/sample - loss: 0.1668 - acc: 0.9577 - val_loss: 0.1975 - val_acc: 0.9313\n",
            "Epoch 14/20\n",
            "520/520 [==============================] - 0s 126us/sample - loss: 0.1537 - acc: 0.9577 - val_loss: 0.1740 - val_acc: 0.9084\n",
            "Epoch 15/20\n",
            "520/520 [==============================] - 0s 118us/sample - loss: 0.1615 - acc: 0.9538 - val_loss: 0.1570 - val_acc: 0.9313\n",
            "Epoch 16/20\n",
            "520/520 [==============================] - 0s 116us/sample - loss: 0.1402 - acc: 0.9673 - val_loss: 0.1577 - val_acc: 0.9466\n",
            "Epoch 17/20\n",
            "520/520 [==============================] - 0s 111us/sample - loss: 0.1364 - acc: 0.9577 - val_loss: 0.1594 - val_acc: 0.9237\n",
            "Epoch 18/20\n",
            "520/520 [==============================] - 0s 120us/sample - loss: 0.1237 - acc: 0.9673 - val_loss: 0.1747 - val_acc: 0.9313\n",
            "Epoch 19/20\n",
            "520/520 [==============================] - 0s 111us/sample - loss: 0.1253 - acc: 0.9750 - val_loss: 0.1613 - val_acc: 0.9466\n",
            "Epoch 20/20\n",
            "520/520 [==============================] - 0s 108us/sample - loss: 0.1189 - acc: 0.9673 - val_loss: 0.2931 - val_acc: 0.8855\n",
            "163/163 [==============================] - 0s 121us/sample - loss: 0.3558 - acc: 0.8466\n",
            "Validation Loss:0.2931133911354851, Validation Accuracy:0.885496199131012, Test Accuracy:0.8466257452964783 with epochs:20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFnCAYAAACPasF4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwmD+H1gPc0RAmKW1AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "FfotsrKb2KSA",
        "colab_type": "code",
        "outputId": "8ce2bdcf-0fa9-471f-cef8-a04f469373ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install hpbandster"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hpbandster\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/3e/62192b0bb527d9353d222b4b6df14400b3c4f36a92a2b138f11a5eafe000/hpbandster-0.7.4.tar.gz (51kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 5.6MB/s \n",
            "\u001b[?25hCollecting Pyro4 (from hpbandster)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/93/219968d8bdc3eab73b8971b155b2006e11cf1e0b284d2e81611da8c3e7ca/Pyro4-4.75-py2.py3-none-any.whl (89kB)\n",
            "\u001b[K    100% |████████████████████████████████| 92kB 13.2MB/s \n",
            "\u001b[?25hCollecting serpent (from hpbandster)\n",
            "  Downloading https://files.pythonhosted.org/packages/27/8a/873ccbe1d3d0f81d136686e4d0f38619ac1e718cff7d68f80e364dc52a8c/serpent-1.28-py2.py3-none-any.whl\n",
            "Collecting ConfigSpace (from hpbandster)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/de/4e8e4f26332fc65404f52baa112defbf822b6738b60bfa6b2993f5c60933/ConfigSpace-0.4.10.tar.gz (882kB)\n",
            "\u001b[K    100% |████████████████████████████████| 890kB 17.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hpbandster) (1.14.6)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.6/dist-packages (from hpbandster) (0.8.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hpbandster) (1.1.0)\n",
            "Collecting netifaces (from hpbandster)\n",
            "  Downloading https://files.pythonhosted.org/packages/0c/9b/c4c7eb09189548d45939a3d3a6b3d53979c67d124459b27a094c365c347f/netifaces-0.10.9-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from ConfigSpace->hpbandster) (2.3.1)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from ConfigSpace->hpbandster) (3.6.6)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from ConfigSpace->hpbandster) (0.29.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from statsmodels->hpbandster) (0.22.0)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.6/dist-packages (from statsmodels->hpbandster) (0.5.1)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->statsmodels->hpbandster) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas->statsmodels->hpbandster) (2.5.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy->statsmodels->hpbandster) (1.11.0)\n",
            "Building wheels for collected packages: hpbandster, ConfigSpace\n",
            "  Building wheel for hpbandster (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/9d/57/62/6b00c8011bac96e0c404adc5be4e16964ba4544614240b4e23\n",
            "  Building wheel for ConfigSpace (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/75/83/cb/28dd42bac69c8867d485138030daa83841c7f84afe68b2fdf7\n",
            "Successfully built hpbandster ConfigSpace\n",
            "Installing collected packages: serpent, Pyro4, ConfigSpace, netifaces, hpbandster\n",
            "Successfully installed ConfigSpace-0.4.10 Pyro4-4.75 hpbandster-0.7.4 netifaces-0.10.9 serpent-1.28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "o7CP7lpe2M1k",
        "colab_type": "code",
        "outputId": "1f5bf43d-7c28-4f45-8a52-bcd62a69fe65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
        "from keras import backend as K\n",
        "from keras import regularizers\n",
        "\n",
        "import datetime\n",
        "import pickle\n",
        "\n",
        "import os\n",
        "import logging\n",
        "\n",
        "import ConfigSpace as CS\n",
        "import ConfigSpace.hyperparameters as CSH\n",
        "from hpbandster.core.worker import Worker\n",
        "\n",
        "import hpbandster.core.result as hpres\n",
        "# import hpbandster.visualization as hpvis\n",
        "import hpbandster.core.nameserver as hpns\n",
        "from hpbandster.optimizers import BOHB\n",
        "\n",
        "\n",
        "class KerasWorker(Worker):\n",
        "    def __init__(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Initialise class and store training and testing data\n",
        "\n",
        "        source: source of csv file (excel or nyone)\n",
        "        \"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "        \n",
        "        df1 = pd.read_csv(\"BC_Data1.csv\")\n",
        "        X = pd.DataFrame(data=df1,columns= ['Age', 'Tumor Size','Nodes','KI67','Basal-like Score','Luminal A Score','Luminal B Score', 'HER2-enriched Score',\n",
        "                                    'Normal Score','ESR1 Score','ERBB2 Score','PGR Score','Proliferation Score','Luminal Score',\n",
        "                                    'ACTR3B','ANLN','BAG1','BCL2','BLVRA','CCNE1','CDC6','CDH3','CXXC5', 'EGFR' ,'ERBB2', 'ESR1','EXO1', 'FGFR4',\n",
        "                                    'FOXA1', 'FOXC1', 'GRB7', 'KRT14', 'KRT17','KRT5', 'MAPT','MDM2','MIA','MLPH','MMP11', 'MYBL2', 'MYC', 'NAT1',\n",
        "                                    'ORC6L', 'PGR', 'PHGDH', 'SFRP1' , 'SLC39A6', 'UBE2T' ,'CDC20' ,'MKI67', 'RRM2','TYMS', 'UBE2C', 'CENPF', 'GPR160',\n",
        "                                    'KIF2C', 'MELK', 'TMEM45B', 'BIRC5', 'CCNB1','CDCA1','CEP55', 'KNTC2','PTTG1'])\n",
        "        X['KI67'] = X['KI67'].fillna(0)\n",
        "        y = pd.DataFrame(data=df1,columns= ['Subtype Prediction'])\n",
        "        X = np.array(X).astype(np.float64)\n",
        "        y = np.array(y)\n",
        "        one = OneHotEncoder() #initiate class\n",
        "        y = one.fit_transform(y)\n",
        "        X,y = shuffle(X,y)\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "        X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1)\n",
        "        self.X_train, self.X_test, self.X_valid, self.y_train, self.y_test, self.y_valid = X_train, X_test, X_valid, y_train, y_test, y_valid\n",
        "\n",
        "    def compute(self, config, budget, working_directory, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Get model with hyperparameters from config generated by get_configspace()\n",
        "        \"\"\"\n",
        "\n",
        "        K.clear_session()  # to reset the network graph for each model\n",
        "\n",
        "        a = datetime.datetime.now()\n",
        "\n",
        "        budget = int(budget)\n",
        "        model = Sequential()\n",
        "        model.add(Dense(int(config['dense_units']), input_dim=64, kernel_initializer='normal', activation=config['activation']))\n",
        "        for i in range(0,config['number_of_layers']):\n",
        "          model.add(Dense(int(config['dense_units']), activation=config['activation']))\n",
        "        model.add(Dense(5, activation='softmax'))\n",
        "        \n",
        "        if config['optimizer'] == 'Nadam':\n",
        "            optimizer = keras.optimizers.Nadam(config['lr'])\n",
        "        elif config['optimizer'] == 'SGD':\n",
        "          \n",
        "            optimizer = keras.optimizers.SGD(config['lr'], config['sgd_momentum'])\n",
        "        elif config['optimizer'] == 'adam':\n",
        "            optimizer = keras.optimizers.adam(config['lr'])\n",
        "        model.compile(loss=keras.losses.categorical_crossentropy, optimizer=optimizer, metrics=['accuracy'])\n",
        "        history = model.fit(self.X_train, self.y_train, epochs=budget, batch_size=config['batch_size'],  verbose=1, validation_data=(self.X_valid, self.y_valid))\n",
        "        testing_score = model.evaluate(self.X_test, self.y_test)\n",
        "        c = datetime.datetime.now() - a\n",
        "        \n",
        "#         print(f'Validation Loss:{history.history['val_loss']} with epochs:{budget}, in :{c} seconds')\n",
        "\n",
        "        return({\n",
        "            'loss': history.history['val_loss'][-1],  # Hyperband always minimizes, so we want to minimise the error, error = 1-accuracy\n",
        "            'info': {'dataset_used': 'Proteomes', 'test_accuracy': testing_score}  # mandatory- can be used in the future to give more information\n",
        "        })\n",
        "\n",
        "        \n",
        "    @staticmethod\n",
        "    def get_configspace():\n",
        "        \"\"\"\n",
        "        Define all the hyperparameters that need to be optimised and store them in config\n",
        "        \"\"\"\n",
        "        cs = CS.ConfigurationSpace()\n",
        "\n",
        "        lr = CSH.UniformFloatHyperparameter('lr', lower=1e-6, upper=1e-1, default_value='1e-2', log=True)\n",
        "        optimizer = CSH.CategoricalHyperparameter('optimizer', ['Nadam', 'SGD','adam'])\n",
        "        sgd_momentum = CSH.UniformFloatHyperparameter('sgd_momentum', lower=0.0, upper=0.99, default_value=0.9, log=False)\n",
        "        batch_size = CSH.UniformIntegerHyperparameter('batch_size', lower=2, upper=6, default_value=4)\n",
        "        cs.add_hyperparameters([lr, optimizer, sgd_momentum, batch_size])\n",
        "        \n",
        "        activation = CSH.CategoricalHyperparameter('activation', ['relu', 'tanh'])\n",
        "        cs.add_hyperparameters([activation])\n",
        "        number_of_layers = CSH.UniformIntegerHyperparameter('number_of_layers', lower=0, upper=4)\n",
        "        cs.add_hyperparameters([number_of_layers])\n",
        "        \n",
        "        dense_units = CSH.UniformIntegerHyperparameter('dense_units', lower=64 , upper=256 , log=True )\n",
        "        cs.add_hyperparameters([dense_units])\n",
        "\n",
        "        cond = CS.EqualsCondition(sgd_momentum, optimizer, 'SGD')\n",
        "        cs.add_condition(cond)\n",
        "    \n",
        "        return cs\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
            "  from pandas.core import datetools\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "eAZG4_d66bvh",
        "colab_type": "code",
        "outputId": "a5831cef-b2d3-4315-a45f-ddaad389bd6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 26778
        }
      },
      "cell_type": "code",
      "source": [
        "def run_bohb(exp_name):\n",
        "    \n",
        "    run_dir='./'\n",
        "    result_logger = hpres.json_result_logger(directory=run_dir, overwrite=True)\n",
        "    # Start a nameserver\n",
        "    NS = hpns.NameServer(run_id=exp_name, host='127.0.0.1', port=0, working_directory=run_dir)\n",
        "    ns_host, ns_port = NS.start()\n",
        "\n",
        "    # Start a localserver\n",
        "    worker = KerasWorker( run_id=exp_name, host='127.0.0.1', nameserver=ns_host, nameserver_port=ns_port, \n",
        "                        timeout=120)\n",
        "    worker.run(background=True)\n",
        "\n",
        "    # Initialise optimiser\n",
        "    bohb = BOHB(configspace=worker.get_configspace(),\n",
        "                run_id=exp_name,\n",
        "                host='127.0.0.1',\n",
        "                nameserver=ns_host, \n",
        "                nameserver_port=ns_port,\n",
        "                result_logger=result_logger,\n",
        "                min_budget=10, max_budget=20,\n",
        "                )\n",
        "    res = bohb.run(n_iterations=20)\n",
        "\n",
        "\n",
        "    # Store the results\n",
        "    with open('result.pkl', 'wb') as f:\n",
        "        pickle.dump(res, f)\n",
        "    \n",
        "\n",
        "\n",
        "    # get all runs\n",
        "    all_runs = res.get_all_runs()\n",
        "\n",
        "    # get id to configuration mapping as dictionary\n",
        "    id2conf = res.get_id2config_mapping()\n",
        "\n",
        "    # get best/incubent run\n",
        "    best_run = res.get_incumbent_id()\n",
        "    best_config = id2conf[best_run]['config']\n",
        "    \n",
        "    print(f\"Best run:{best_config}\")\n",
        "\n",
        "    # Store all run info\n",
        "    f = open('summary.txt', 'w' )\n",
        "    f.write(f\"{all_runs}\")\n",
        "    f.close()\n",
        "        # Shutdown\n",
        "    \n",
        "    bohb.shutdown(shutdown_workers=True)\n",
        "    #NS.shutdown()\n",
        "\n",
        "run_bohb('run_01')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17:30:21 wait_for_workers trying to get the condition\n",
            "17:30:21 DISPATCHER: started the 'discover_worker' thread\n",
            "17:30:21 WORKER: Connected to nameserver <Pyro4.core.Proxy at 0x7f76162a4cf8; connected IPv4; for PYRO:Pyro.NameServer@127.0.0.1:45127>\n",
            "17:30:21 DISPATCHER: started the 'job_runner' thread\n",
            "17:30:21 WORKER: No dispatcher found. Waiting for one to initiate contact.\n",
            "17:30:21 WORKER: start listening for jobs\n",
            "17:30:21 DISPATCHER: Pyro daemon running on 127.0.0.1:37243\n",
            "17:30:21 DISPATCHER: Starting worker discovery\n",
            "17:30:21 DISPATCHER: Found 1 potential workers, 0 currently in the pool.\n",
            "17:30:21 DISPATCHER: discovered new worker, hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:30:21 HBMASTER: number of workers changed to 1\n",
            "17:30:21 Enough workers to start this run!\n",
            "17:30:21 HBMASTER: starting run at 1553707821.387612\n",
            "17:30:21 adjust_queue_size: lock accquired\n",
            "17:30:21 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
            "17:30:21 HBMASTER: adjusted queue size to (0, 1)\n",
            "17:30:21 DISPATCHER: Finished worker discovery\n",
            "17:30:21 start sampling a new configuration.\n",
            "17:30:21 DISPATCHER: Trying to submit another job.\n",
            "17:30:21 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
            "17:30:21 done sampling a new configuration.\n",
            "17:30:21 HBMASTER: schedule new run for iteration 0\n",
            "17:30:21 HBMASTER: trying submitting job (0, 0, 0) to dispatcher\n",
            "17:30:21 HBMASTER: submitting job (0, 0, 0) to dispatcher\n",
            "17:30:21 DISPATCHER: trying to submit job (0, 0, 0)\n",
            "17:30:21 DISPATCHER: trying to notify the job_runner thread.\n",
            "17:30:21 HBMASTER: job (0, 0, 0) submitted to dispatcher\n",
            "17:30:21 DISPATCHER: Trying to submit another job.\n",
            "17:30:21 DISPATCHER: starting job (0, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:30:21 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
            "17:30:21 DISPATCHER: job (0, 0, 0) dispatched on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:30:21 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
            "17:30:21 WORKER: start processing job (0, 0, 0)\n",
            "17:30:21 WORKER: args: ()\n",
            "17:30:21 WORKER: kwargs: {'config': {'activation': 'tanh', 'batch_size': 5, 'dense_units': 81, 'lr': 0.00010897659937884412, 'number_of_layers': 0, 'optimizer': 'SGD', 'sgd_momentum': 0.8807065622180759}, 'budget': 20.0, 'working_directory': '.'}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 585 samples, validate on 66 samples\n",
            "Epoch 1/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 1.2528 - acc: 0.4957 - val_loss: 1.1293 - val_acc: 0.5152\n",
            "Epoch 2/20\n",
            "585/585 [==============================] - 0s 791us/step - loss: 0.9404 - acc: 0.6752 - val_loss: 0.9682 - val_acc: 0.6515\n",
            "Epoch 3/20\n",
            "585/585 [==============================] - 0s 813us/step - loss: 0.8506 - acc: 0.7179 - val_loss: 0.8840 - val_acc: 0.6667\n",
            "Epoch 4/20\n",
            "585/585 [==============================] - 0s 819us/step - loss: 0.7676 - acc: 0.7419 - val_loss: 0.7911 - val_acc: 0.7576\n",
            "Epoch 5/20\n",
            "585/585 [==============================] - 0s 774us/step - loss: 0.6947 - acc: 0.7761 - val_loss: 0.7874 - val_acc: 0.6970\n",
            "Epoch 6/20\n",
            "585/585 [==============================] - 0s 754us/step - loss: 0.6458 - acc: 0.7983 - val_loss: 0.7259 - val_acc: 0.7121\n",
            "Epoch 7/20\n",
            "585/585 [==============================] - 0s 793us/step - loss: 0.6038 - acc: 0.8137 - val_loss: 0.7511 - val_acc: 0.6818\n",
            "Epoch 8/20\n",
            "585/585 [==============================] - 0s 802us/step - loss: 0.5801 - acc: 0.8188 - val_loss: 0.6503 - val_acc: 0.6970\n",
            "Epoch 9/20\n",
            "585/585 [==============================] - 0s 794us/step - loss: 0.5430 - acc: 0.8205 - val_loss: 0.6659 - val_acc: 0.7576\n",
            "Epoch 10/20\n",
            "585/585 [==============================] - 0s 789us/step - loss: 0.5246 - acc: 0.8291 - val_loss: 0.6650 - val_acc: 0.7424\n",
            "Epoch 11/20\n",
            "585/585 [==============================] - 0s 803us/step - loss: 0.5023 - acc: 0.8376 - val_loss: 0.5884 - val_acc: 0.7879\n",
            "Epoch 12/20\n",
            "585/585 [==============================] - 0s 777us/step - loss: 0.4863 - acc: 0.8462 - val_loss: 0.5761 - val_acc: 0.7576\n",
            "Epoch 13/20\n",
            "585/585 [==============================] - 0s 800us/step - loss: 0.4738 - acc: 0.8547 - val_loss: 0.5982 - val_acc: 0.7121\n",
            "Epoch 14/20\n",
            "585/585 [==============================] - 0s 807us/step - loss: 0.4490 - acc: 0.8547 - val_loss: 0.5150 - val_acc: 0.7879\n",
            "Epoch 15/20\n",
            "585/585 [==============================] - 0s 802us/step - loss: 0.4348 - acc: 0.8632 - val_loss: 0.5859 - val_acc: 0.7576\n",
            "Epoch 16/20\n",
            "585/585 [==============================] - 0s 782us/step - loss: 0.4238 - acc: 0.8650 - val_loss: 0.5551 - val_acc: 0.7879\n",
            "Epoch 17/20\n",
            "585/585 [==============================] - 0s 763us/step - loss: 0.4205 - acc: 0.8650 - val_loss: 0.5428 - val_acc: 0.8030\n",
            "Epoch 18/20\n",
            "585/585 [==============================] - 0s 812us/step - loss: 0.4033 - acc: 0.8684 - val_loss: 0.5658 - val_acc: 0.7576\n",
            "Epoch 19/20\n",
            "585/585 [==============================] - 0s 790us/step - loss: 0.3890 - acc: 0.8803 - val_loss: 0.6020 - val_acc: 0.7121\n",
            "Epoch 20/20\n",
            "585/585 [==============================] - 0s 794us/step - loss: 0.3695 - acc: 0.8821 - val_loss: 0.5009 - val_acc: 0.7727\n",
            "163/163 [==============================] - 0s 95us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "17:30:31 WORKER: done with job (0, 0, 0), trying to register it.\n",
            "17:30:31 WORKER: registered result for job (0, 0, 0) with dispatcher\n",
            "17:30:31 DISPATCHER: job (0, 0, 0) finished\n",
            "17:30:31 DISPATCHER: register_result: lock acquired\n",
            "17:30:31 DISPATCHER: job (0, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368 finished\n",
            "17:30:31 job_id: (0, 0, 0)\n",
            "kwargs: {'config': {'activation': 'tanh', 'batch_size': 5, 'dense_units': 81, 'lr': 0.00010897659937884412, 'number_of_layers': 0, 'optimizer': 'SGD', 'sgd_momentum': 0.8807065622180759}, 'budget': 20.0, 'working_directory': '.'}\n",
            "result: {'loss': 0.5009130377209547, 'info': {'dataset_used': 'Proteomes', 'test_accuracy': [0.4393298945909629, 0.8282208588957055]}}\n",
            "exception: None\n",
            "\n",
            "17:30:31 job_callback for (0, 0, 0) started\n",
            "17:30:31 DISPATCHER: Trying to submit another job.\n",
            "17:30:31 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
            "17:30:31 job_callback for (0, 0, 0) got condition\n",
            "17:30:31 Only 1 run(s) for budget 20.000000 available, need more than 9 -> can't build model!\n",
            "17:30:31 HBMASTER: Trying to run another job!\n",
            "17:30:31 job_callback for (0, 0, 0) finished\n",
            "17:30:31 start sampling a new configuration.\n",
            "17:30:31 done sampling a new configuration.\n",
            "17:30:31 HBMASTER: schedule new run for iteration 1\n",
            "17:30:31 HBMASTER: trying submitting job (1, 0, 0) to dispatcher\n",
            "17:30:31 HBMASTER: submitting job (1, 0, 0) to dispatcher\n",
            "17:30:31 DISPATCHER: trying to submit job (1, 0, 0)\n",
            "17:30:31 DISPATCHER: trying to notify the job_runner thread.\n",
            "17:30:31 HBMASTER: job (1, 0, 0) submitted to dispatcher\n",
            "17:30:31 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
            "17:30:31 DISPATCHER: Trying to submit another job.\n",
            "17:30:31 DISPATCHER: starting job (1, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:30:31 DISPATCHER: job (1, 0, 0) dispatched on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:30:31 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
            "17:30:31 WORKER: start processing job (1, 0, 0)\n",
            "17:30:31 WORKER: args: ()\n",
            "17:30:31 WORKER: kwargs: {'config': {'activation': 'relu', 'batch_size': 6, 'dense_units': 97, 'lr': 0.0794861031325187, 'number_of_layers': 0, 'optimizer': 'SGD', 'sgd_momentum': 0.15614617271590037}, 'budget': 20.0, 'working_directory': '.'}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 585 samples, validate on 66 samples\n",
            "Epoch 1/20\n",
            "585/585 [==============================] - 1s 922us/step - loss: 10.8295 - acc: 0.3197 - val_loss: 11.4780 - val_acc: 0.2879\n",
            "Epoch 2/20\n",
            "585/585 [==============================] - 0s 639us/step - loss: 10.9934 - acc: 0.3179 - val_loss: 11.4780 - val_acc: 0.2879\n",
            "Epoch 3/20\n",
            "585/585 [==============================] - 0s 684us/step - loss: 10.9934 - acc: 0.3179 - val_loss: 11.4780 - val_acc: 0.2879\n",
            "Epoch 4/20\n",
            "585/585 [==============================] - 0s 689us/step - loss: 10.9934 - acc: 0.3179 - val_loss: 11.4780 - val_acc: 0.2879\n",
            "Epoch 5/20\n",
            "585/585 [==============================] - 0s 641us/step - loss: 10.9934 - acc: 0.3179 - val_loss: 11.4780 - val_acc: 0.2879\n",
            "Epoch 6/20\n",
            "585/585 [==============================] - 0s 647us/step - loss: 10.9934 - acc: 0.3179 - val_loss: 11.4780 - val_acc: 0.2879\n",
            "Epoch 7/20\n",
            "585/585 [==============================] - 0s 645us/step - loss: 10.9934 - acc: 0.3179 - val_loss: 11.4780 - val_acc: 0.2879\n",
            "Epoch 8/20\n",
            "585/585 [==============================] - 0s 632us/step - loss: 10.9934 - acc: 0.3179 - val_loss: 11.4780 - val_acc: 0.2879\n",
            "Epoch 9/20\n",
            "585/585 [==============================] - 0s 724us/step - loss: 10.9934 - acc: 0.3179 - val_loss: 11.4780 - val_acc: 0.2879\n",
            "Epoch 10/20\n",
            "585/585 [==============================] - 0s 690us/step - loss: 10.9934 - acc: 0.3179 - val_loss: 11.4780 - val_acc: 0.2879\n",
            "Epoch 11/20\n",
            "585/585 [==============================] - 0s 660us/step - loss: 10.9934 - acc: 0.3179 - val_loss: 11.4780 - val_acc: 0.2879\n",
            "Epoch 12/20\n",
            "585/585 [==============================] - 0s 691us/step - loss: 10.9934 - acc: 0.3179 - val_loss: 11.4780 - val_acc: 0.2879\n",
            "Epoch 13/20\n",
            "585/585 [==============================] - 0s 680us/step - loss: 10.9934 - acc: 0.3179 - val_loss: 11.4780 - val_acc: 0.2879\n",
            "Epoch 14/20\n",
            "585/585 [==============================] - 0s 674us/step - loss: 10.9934 - acc: 0.3179 - val_loss: 11.4780 - val_acc: 0.2879\n",
            "Epoch 15/20\n",
            "585/585 [==============================] - 0s 636us/step - loss: 10.9934 - acc: 0.3179 - val_loss: 11.4780 - val_acc: 0.2879\n",
            "Epoch 16/20\n",
            "585/585 [==============================] - 0s 682us/step - loss: 10.9934 - acc: 0.3179 - val_loss: 11.4780 - val_acc: 0.2879\n",
            "Epoch 17/20\n",
            "585/585 [==============================] - 0s 701us/step - loss: 10.9934 - acc: 0.3179 - val_loss: 11.4780 - val_acc: 0.2879\n",
            "Epoch 18/20\n",
            "585/585 [==============================] - 0s 678us/step - loss: 10.9934 - acc: 0.3179 - val_loss: 11.4780 - val_acc: 0.2879\n",
            "Epoch 19/20\n",
            "585/585 [==============================] - 0s 720us/step - loss: 10.9934 - acc: 0.3179 - val_loss: 11.4780 - val_acc: 0.2879\n",
            "Epoch 20/20\n",
            "585/585 [==============================] - 0s 703us/step - loss: 10.9934 - acc: 0.3179 - val_loss: 11.4780 - val_acc: 0.2879\n",
            "163/163 [==============================] - 0s 101us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "17:30:39 WORKER: done with job (1, 0, 0), trying to register it.\n",
            "17:30:39 WORKER: registered result for job (1, 0, 0) with dispatcher\n",
            "17:30:39 DISPATCHER: job (1, 0, 0) finished\n",
            "17:30:39 DISPATCHER: register_result: lock acquired\n",
            "17:30:39 DISPATCHER: job (1, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368 finished\n",
            "17:30:39 job_id: (1, 0, 0)\n",
            "kwargs: {'config': {'activation': 'relu', 'batch_size': 6, 'dense_units': 97, 'lr': 0.0794861031325187, 'number_of_layers': 0, 'optimizer': 'SGD', 'sgd_momentum': 0.15614617271590037}, 'budget': 20.0, 'working_directory': '.'}\n",
            "result: {'loss': 11.478037574074484, 'info': {'dataset_used': 'Proteomes', 'test_accuracy': [10.580590324167824, 0.34355828220858897]}}\n",
            "exception: None\n",
            "\n",
            "17:30:39 job_callback for (1, 0, 0) started\n",
            "17:30:39 DISPATCHER: Trying to submit another job.\n",
            "17:30:39 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
            "17:30:39 job_callback for (1, 0, 0) got condition\n",
            "17:30:39 Only 2 run(s) for budget 20.000000 available, need more than 9 -> can't build model!\n",
            "17:30:39 HBMASTER: Trying to run another job!\n",
            "17:30:39 job_callback for (1, 0, 0) finished\n",
            "17:30:39 start sampling a new configuration.\n",
            "17:30:39 done sampling a new configuration.\n",
            "17:30:39 HBMASTER: schedule new run for iteration 2\n",
            "17:30:39 HBMASTER: trying submitting job (2, 0, 0) to dispatcher\n",
            "17:30:39 HBMASTER: submitting job (2, 0, 0) to dispatcher\n",
            "17:30:39 DISPATCHER: trying to submit job (2, 0, 0)\n",
            "17:30:39 DISPATCHER: trying to notify the job_runner thread.\n",
            "17:30:39 HBMASTER: job (2, 0, 0) submitted to dispatcher\n",
            "17:30:39 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
            "17:30:39 DISPATCHER: Trying to submit another job.\n",
            "17:30:39 DISPATCHER: starting job (2, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:30:39 DISPATCHER: job (2, 0, 0) dispatched on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:30:39 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
            "17:30:39 WORKER: start processing job (2, 0, 0)\n",
            "17:30:39 WORKER: args: ()\n",
            "17:30:39 WORKER: kwargs: {'config': {'activation': 'tanh', 'batch_size': 3, 'dense_units': 68, 'lr': 0.008591623706615167, 'number_of_layers': 1, 'optimizer': 'Nadam'}, 'budget': 20.0, 'working_directory': '.'}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 585 samples, validate on 66 samples\n",
            "Epoch 1/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.8950 - acc: 0.6462 - val_loss: 0.6832 - val_acc: 0.7424\n",
            "Epoch 2/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.5730 - acc: 0.7692 - val_loss: 0.3393 - val_acc: 0.8788\n",
            "Epoch 3/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.6430 - acc: 0.7556 - val_loss: 0.7456 - val_acc: 0.7121\n",
            "Epoch 4/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.5244 - acc: 0.8137 - val_loss: 1.0224 - val_acc: 0.6818\n",
            "Epoch 5/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.5281 - acc: 0.7949 - val_loss: 0.8024 - val_acc: 0.6818\n",
            "Epoch 6/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.4453 - acc: 0.8427 - val_loss: 0.7024 - val_acc: 0.7121\n",
            "Epoch 7/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.5427 - acc: 0.8017 - val_loss: 0.6000 - val_acc: 0.7121\n",
            "Epoch 8/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.5091 - acc: 0.8222 - val_loss: 0.3815 - val_acc: 0.8333\n",
            "Epoch 9/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.4188 - acc: 0.8427 - val_loss: 0.7189 - val_acc: 0.7121\n",
            "Epoch 10/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.4940 - acc: 0.8085 - val_loss: 0.3110 - val_acc: 0.8485\n",
            "Epoch 11/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.4777 - acc: 0.8256 - val_loss: 0.6738 - val_acc: 0.7121\n",
            "Epoch 12/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.5609 - acc: 0.7812 - val_loss: 0.2991 - val_acc: 0.8939\n",
            "Epoch 13/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.5327 - acc: 0.7915 - val_loss: 0.4406 - val_acc: 0.8788\n",
            "Epoch 14/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.4451 - acc: 0.8513 - val_loss: 0.6685 - val_acc: 0.6818\n",
            "Epoch 15/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.5076 - acc: 0.8051 - val_loss: 0.4821 - val_acc: 0.7879\n",
            "Epoch 16/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.4563 - acc: 0.8342 - val_loss: 0.3415 - val_acc: 0.8788\n",
            "Epoch 17/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.3968 - acc: 0.8786 - val_loss: 0.3754 - val_acc: 0.8333\n",
            "Epoch 18/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.5381 - acc: 0.7812 - val_loss: 0.6553 - val_acc: 0.6818\n",
            "Epoch 19/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.5486 - acc: 0.7897 - val_loss: 0.7962 - val_acc: 0.6667\n",
            "Epoch 20/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.5260 - acc: 0.8205 - val_loss: 0.6159 - val_acc: 0.7727\n",
            "163/163 [==============================] - 0s 100us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "17:31:02 WORKER: done with job (2, 0, 0), trying to register it.\n",
            "17:31:02 WORKER: registered result for job (2, 0, 0) with dispatcher\n",
            "17:31:02 DISPATCHER: job (2, 0, 0) finished\n",
            "17:31:02 DISPATCHER: register_result: lock acquired\n",
            "17:31:02 DISPATCHER: job (2, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368 finished\n",
            "17:31:02 job_id: (2, 0, 0)\n",
            "kwargs: {'config': {'activation': 'tanh', 'batch_size': 3, 'dense_units': 68, 'lr': 0.008591623706615167, 'number_of_layers': 1, 'optimizer': 'Nadam'}, 'budget': 20.0, 'working_directory': '.'}\n",
            "result: {'loss': 0.6158586383204568, 'info': {'dataset_used': 'Proteomes', 'test_accuracy': [0.7997019429879686, 0.7546012273595377]}}\n",
            "exception: None\n",
            "\n",
            "17:31:02 job_callback for (2, 0, 0) started\n",
            "17:31:02 job_callback for (2, 0, 0) got condition\n",
            "17:31:02 DISPATCHER: Trying to submit another job.\n",
            "17:31:02 Only 3 run(s) for budget 20.000000 available, need more than 9 -> can't build model!\n",
            "17:31:02 HBMASTER: Trying to run another job!\n",
            "17:31:02 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
            "17:31:02 job_callback for (2, 0, 0) finished\n",
            "17:31:02 start sampling a new configuration.\n",
            "17:31:02 done sampling a new configuration.\n",
            "17:31:02 HBMASTER: schedule new run for iteration 3\n",
            "17:31:02 HBMASTER: trying submitting job (3, 0, 0) to dispatcher\n",
            "17:31:02 HBMASTER: submitting job (3, 0, 0) to dispatcher\n",
            "17:31:02 DISPATCHER: trying to submit job (3, 0, 0)\n",
            "17:31:02 DISPATCHER: trying to notify the job_runner thread.\n",
            "17:31:02 HBMASTER: job (3, 0, 0) submitted to dispatcher\n",
            "17:31:02 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
            "17:31:02 DISPATCHER: Trying to submit another job.\n",
            "17:31:02 DISPATCHER: starting job (3, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:31:02 DISPATCHER: job (3, 0, 0) dispatched on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:31:02 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
            "17:31:02 WORKER: start processing job (3, 0, 0)\n",
            "17:31:02 WORKER: args: ()\n",
            "17:31:02 WORKER: kwargs: {'config': {'activation': 'tanh', 'batch_size': 6, 'dense_units': 78, 'lr': 0.03414025604154261, 'number_of_layers': 0, 'optimizer': 'Nadam'}, 'budget': 20.0, 'working_directory': '.'}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 585 samples, validate on 66 samples\n",
            "Epoch 1/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 1.4152 - acc: 0.4957 - val_loss: 1.7169 - val_acc: 0.3636\n",
            "Epoch 2/20\n",
            "585/585 [==============================] - 0s 822us/step - loss: 1.1796 - acc: 0.5692 - val_loss: 1.0867 - val_acc: 0.5000\n",
            "Epoch 3/20\n",
            "585/585 [==============================] - 0s 809us/step - loss: 1.0404 - acc: 0.6085 - val_loss: 0.8613 - val_acc: 0.5909\n",
            "Epoch 4/20\n",
            "585/585 [==============================] - 0s 834us/step - loss: 0.9964 - acc: 0.6137 - val_loss: 2.9417 - val_acc: 0.3182\n",
            "Epoch 5/20\n",
            "585/585 [==============================] - 0s 822us/step - loss: 1.0695 - acc: 0.6462 - val_loss: 1.5441 - val_acc: 0.5303\n",
            "Epoch 6/20\n",
            "585/585 [==============================] - 0s 820us/step - loss: 1.4195 - acc: 0.4991 - val_loss: 1.4369 - val_acc: 0.3182\n",
            "Epoch 7/20\n",
            "585/585 [==============================] - 0s 809us/step - loss: 1.1711 - acc: 0.5778 - val_loss: 1.9599 - val_acc: 0.2727\n",
            "Epoch 8/20\n",
            "585/585 [==============================] - 0s 824us/step - loss: 1.1490 - acc: 0.5795 - val_loss: 1.0601 - val_acc: 0.5758\n",
            "Epoch 9/20\n",
            "585/585 [==============================] - 0s 805us/step - loss: 1.0507 - acc: 0.6410 - val_loss: 1.2658 - val_acc: 0.5909\n",
            "Epoch 10/20\n",
            "585/585 [==============================] - 0s 818us/step - loss: 1.0701 - acc: 0.5915 - val_loss: 2.0539 - val_acc: 0.4091\n",
            "Epoch 11/20\n",
            "585/585 [==============================] - 0s 808us/step - loss: 1.0813 - acc: 0.5795 - val_loss: 0.9877 - val_acc: 0.5909\n",
            "Epoch 12/20\n",
            "585/585 [==============================] - 0s 822us/step - loss: 0.9010 - acc: 0.6752 - val_loss: 0.9518 - val_acc: 0.6515\n",
            "Epoch 13/20\n",
            "585/585 [==============================] - 0s 796us/step - loss: 0.8311 - acc: 0.7282 - val_loss: 1.0435 - val_acc: 0.6667\n",
            "Epoch 14/20\n",
            "585/585 [==============================] - 0s 820us/step - loss: 0.9337 - acc: 0.6821 - val_loss: 1.3042 - val_acc: 0.5909\n",
            "Epoch 15/20\n",
            "585/585 [==============================] - 0s 825us/step - loss: 0.8097 - acc: 0.7316 - val_loss: 0.7264 - val_acc: 0.6818\n",
            "Epoch 16/20\n",
            "585/585 [==============================] - 0s 820us/step - loss: 0.8371 - acc: 0.6957 - val_loss: 1.1385 - val_acc: 0.5606\n",
            "Epoch 17/20\n",
            "585/585 [==============================] - 0s 813us/step - loss: 0.9060 - acc: 0.6718 - val_loss: 0.9780 - val_acc: 0.6818\n",
            "Epoch 18/20\n",
            "585/585 [==============================] - 0s 805us/step - loss: 0.9707 - acc: 0.6513 - val_loss: 1.5371 - val_acc: 0.4848\n",
            "Epoch 19/20\n",
            "585/585 [==============================] - 0s 823us/step - loss: 1.1851 - acc: 0.5538 - val_loss: 1.1879 - val_acc: 0.6818\n",
            "Epoch 20/20\n",
            "585/585 [==============================] - 0s 812us/step - loss: 0.8913 - acc: 0.7231 - val_loss: 1.0758 - val_acc: 0.7121\n",
            "163/163 [==============================] - 0s 93us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "17:31:12 WORKER: done with job (3, 0, 0), trying to register it.\n",
            "17:31:12 WORKER: registered result for job (3, 0, 0) with dispatcher\n",
            "17:31:12 DISPATCHER: job (3, 0, 0) finished\n",
            "17:31:12 DISPATCHER: register_result: lock acquired\n",
            "17:31:12 DISPATCHER: job (3, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368 finished\n",
            "17:31:12 job_id: (3, 0, 0)\n",
            "kwargs: {'config': {'activation': 'tanh', 'batch_size': 6, 'dense_units': 78, 'lr': 0.03414025604154261, 'number_of_layers': 0, 'optimizer': 'Nadam'}, 'budget': 20.0, 'working_directory': '.'}\n",
            "result: {'loss': 1.0758239267901941, 'info': {'dataset_used': 'Proteomes', 'test_accuracy': [1.3217051848312098, 0.6441717793239407]}}\n",
            "exception: None\n",
            "\n",
            "17:31:12 job_callback for (3, 0, 0) started\n",
            "17:31:12 DISPATCHER: Trying to submit another job.\n",
            "17:31:12 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
            "17:31:12 job_callback for (3, 0, 0) got condition\n",
            "17:31:12 Only 4 run(s) for budget 20.000000 available, need more than 9 -> can't build model!\n",
            "17:31:12 HBMASTER: Trying to run another job!\n",
            "17:31:12 job_callback for (3, 0, 0) finished\n",
            "17:31:12 start sampling a new configuration.\n",
            "17:31:12 done sampling a new configuration.\n",
            "17:31:12 HBMASTER: schedule new run for iteration 4\n",
            "17:31:12 HBMASTER: trying submitting job (4, 0, 0) to dispatcher\n",
            "17:31:12 HBMASTER: submitting job (4, 0, 0) to dispatcher\n",
            "17:31:12 DISPATCHER: trying to submit job (4, 0, 0)\n",
            "17:31:12 DISPATCHER: trying to notify the job_runner thread.\n",
            "17:31:12 HBMASTER: job (4, 0, 0) submitted to dispatcher\n",
            "17:31:12 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
            "17:31:12 DISPATCHER: Trying to submit another job.\n",
            "17:31:12 DISPATCHER: starting job (4, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:31:12 DISPATCHER: job (4, 0, 0) dispatched on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:31:12 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
            "17:31:12 WORKER: start processing job (4, 0, 0)\n",
            "17:31:12 WORKER: args: ()\n",
            "17:31:12 WORKER: kwargs: {'config': {'activation': 'tanh', 'batch_size': 6, 'dense_units': 110, 'lr': 0.04425612646055164, 'number_of_layers': 1, 'optimizer': 'SGD', 'sgd_momentum': 0.0033595150632530512}, 'budget': 20.0, 'working_directory': '.'}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 585 samples, validate on 66 samples\n",
            "Epoch 1/20\n",
            "585/585 [==============================] - 1s 972us/step - loss: 1.5735 - acc: 0.3453 - val_loss: 1.6516 - val_acc: 0.1515\n",
            "Epoch 2/20\n",
            "585/585 [==============================] - 0s 698us/step - loss: 1.3188 - acc: 0.4376 - val_loss: 1.2333 - val_acc: 0.5455\n",
            "Epoch 3/20\n",
            "585/585 [==============================] - 0s 696us/step - loss: 1.3273 - acc: 0.4513 - val_loss: 1.5776 - val_acc: 0.3788\n",
            "Epoch 4/20\n",
            "585/585 [==============================] - 0s 712us/step - loss: 1.2518 - acc: 0.4615 - val_loss: 1.8281 - val_acc: 0.2576\n",
            "Epoch 5/20\n",
            "585/585 [==============================] - 0s 676us/step - loss: 1.1830 - acc: 0.5009 - val_loss: 1.3745 - val_acc: 0.4394\n",
            "Epoch 6/20\n",
            "585/585 [==============================] - 0s 680us/step - loss: 1.1205 - acc: 0.5179 - val_loss: 0.9982 - val_acc: 0.5303\n",
            "Epoch 7/20\n",
            "585/585 [==============================] - 0s 699us/step - loss: 1.0611 - acc: 0.5402 - val_loss: 1.4862 - val_acc: 0.4545\n",
            "Epoch 8/20\n",
            "585/585 [==============================] - 0s 706us/step - loss: 0.9705 - acc: 0.6103 - val_loss: 2.2144 - val_acc: 0.1818\n",
            "Epoch 9/20\n",
            "585/585 [==============================] - 0s 710us/step - loss: 0.9632 - acc: 0.6222 - val_loss: 1.0916 - val_acc: 0.5000\n",
            "Epoch 10/20\n",
            "585/585 [==============================] - 0s 691us/step - loss: 0.9889 - acc: 0.5607 - val_loss: 1.5185 - val_acc: 0.4242\n",
            "Epoch 11/20\n",
            "585/585 [==============================] - 0s 687us/step - loss: 0.9712 - acc: 0.5812 - val_loss: 0.8244 - val_acc: 0.6515\n",
            "Epoch 12/20\n",
            "585/585 [==============================] - 0s 706us/step - loss: 0.9068 - acc: 0.6427 - val_loss: 1.8004 - val_acc: 0.2879\n",
            "Epoch 13/20\n",
            "585/585 [==============================] - 0s 694us/step - loss: 0.9647 - acc: 0.5709 - val_loss: 1.5586 - val_acc: 0.3939\n",
            "Epoch 14/20\n",
            "585/585 [==============================] - 0s 695us/step - loss: 1.0320 - acc: 0.5761 - val_loss: 1.5384 - val_acc: 0.3636\n",
            "Epoch 15/20\n",
            "585/585 [==============================] - 0s 695us/step - loss: 0.9275 - acc: 0.5949 - val_loss: 0.9528 - val_acc: 0.6818\n",
            "Epoch 16/20\n",
            "585/585 [==============================] - 0s 698us/step - loss: 0.9275 - acc: 0.5949 - val_loss: 1.0635 - val_acc: 0.5303\n",
            "Epoch 17/20\n",
            "585/585 [==============================] - 0s 709us/step - loss: 0.9333 - acc: 0.5658 - val_loss: 0.7442 - val_acc: 0.6515\n",
            "Epoch 18/20\n",
            "585/585 [==============================] - 0s 681us/step - loss: 0.8609 - acc: 0.6496 - val_loss: 0.7489 - val_acc: 0.6667\n",
            "Epoch 19/20\n",
            "585/585 [==============================] - 0s 712us/step - loss: 0.8582 - acc: 0.6410 - val_loss: 0.8580 - val_acc: 0.5606\n",
            "Epoch 20/20\n",
            " 90/585 [===>..........................] - ETA: 0s - loss: 0.6800 - acc: 0.7111"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "17:31:21 DISPATCHER: Starting worker discovery\n",
            "17:31:21 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
            "17:31:21 DISPATCHER: Finished worker discovery\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "585/585 [==============================] - 0s 700us/step - loss: 0.8856 - acc: 0.6034 - val_loss: 1.5176 - val_acc: 0.4697\n",
            "163/163 [==============================] - 0s 101us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "17:31:21 WORKER: done with job (4, 0, 0), trying to register it.\n",
            "17:31:21 WORKER: registered result for job (4, 0, 0) with dispatcher\n",
            "17:31:21 DISPATCHER: job (4, 0, 0) finished\n",
            "17:31:21 DISPATCHER: register_result: lock acquired\n",
            "17:31:21 DISPATCHER: job (4, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368 finished\n",
            "17:31:21 job_id: (4, 0, 0)\n",
            "kwargs: {'config': {'activation': 'tanh', 'batch_size': 6, 'dense_units': 110, 'lr': 0.04425612646055164, 'number_of_layers': 1, 'optimizer': 'SGD', 'sgd_momentum': 0.0033595150632530512}, 'budget': 20.0, 'working_directory': '.'}\n",
            "result: {'loss': 1.5175600268624045, 'info': {'dataset_used': 'Proteomes', 'test_accuracy': [1.569190060434166, 0.4417177915938793]}}\n",
            "exception: None\n",
            "\n",
            "17:31:21 job_callback for (4, 0, 0) started\n",
            "17:31:21 job_callback for (4, 0, 0) got condition\n",
            "17:31:21 DISPATCHER: Trying to submit another job.\n",
            "17:31:21 Only 5 run(s) for budget 20.000000 available, need more than 9 -> can't build model!\n",
            "17:31:21 HBMASTER: Trying to run another job!\n",
            "17:31:21 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
            "17:31:21 job_callback for (4, 0, 0) finished\n",
            "17:31:21 start sampling a new configuration.\n",
            "17:31:21 done sampling a new configuration.\n",
            "17:31:21 HBMASTER: schedule new run for iteration 5\n",
            "17:31:21 HBMASTER: trying submitting job (5, 0, 0) to dispatcher\n",
            "17:31:21 HBMASTER: submitting job (5, 0, 0) to dispatcher\n",
            "17:31:21 DISPATCHER: trying to submit job (5, 0, 0)\n",
            "17:31:21 DISPATCHER: trying to notify the job_runner thread.\n",
            "17:31:21 HBMASTER: job (5, 0, 0) submitted to dispatcher\n",
            "17:31:21 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
            "17:31:21 DISPATCHER: Trying to submit another job.\n",
            "17:31:21 DISPATCHER: starting job (5, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:31:21 DISPATCHER: job (5, 0, 0) dispatched on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:31:21 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
            "17:31:21 WORKER: start processing job (5, 0, 0)\n",
            "17:31:21 WORKER: args: ()\n",
            "17:31:21 WORKER: kwargs: {'config': {'activation': 'tanh', 'batch_size': 5, 'dense_units': 233, 'lr': 4.824982802774297e-05, 'number_of_layers': 0, 'optimizer': 'adam'}, 'budget': 20.0, 'working_directory': '.'}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 585 samples, validate on 66 samples\n",
            "Epoch 1/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 1.2563 - acc: 0.4923 - val_loss: 1.1522 - val_acc: 0.4848\n",
            "Epoch 2/20\n",
            "585/585 [==============================] - 1s 902us/step - loss: 1.0033 - acc: 0.6239 - val_loss: 0.9680 - val_acc: 0.6970\n",
            "Epoch 3/20\n",
            "585/585 [==============================] - 1s 883us/step - loss: 0.8588 - acc: 0.7111 - val_loss: 0.8493 - val_acc: 0.6818\n",
            "Epoch 4/20\n",
            "585/585 [==============================] - 1s 914us/step - loss: 0.7505 - acc: 0.7453 - val_loss: 0.7246 - val_acc: 0.7576\n",
            "Epoch 5/20\n",
            "585/585 [==============================] - 1s 889us/step - loss: 0.6637 - acc: 0.7846 - val_loss: 0.6445 - val_acc: 0.7576\n",
            "Epoch 6/20\n",
            "585/585 [==============================] - 1s 923us/step - loss: 0.5956 - acc: 0.8017 - val_loss: 0.5788 - val_acc: 0.7727\n",
            "Epoch 7/20\n",
            "585/585 [==============================] - 1s 900us/step - loss: 0.5377 - acc: 0.8291 - val_loss: 0.5218 - val_acc: 0.7879\n",
            "Epoch 8/20\n",
            "585/585 [==============================] - 1s 907us/step - loss: 0.4946 - acc: 0.8359 - val_loss: 0.5020 - val_acc: 0.8182\n",
            "Epoch 9/20\n",
            "585/585 [==============================] - 1s 884us/step - loss: 0.4606 - acc: 0.8530 - val_loss: 0.4602 - val_acc: 0.8333\n",
            "Epoch 10/20\n",
            "585/585 [==============================] - 1s 921us/step - loss: 0.4304 - acc: 0.8735 - val_loss: 0.4390 - val_acc: 0.8485\n",
            "Epoch 11/20\n",
            "585/585 [==============================] - 1s 928us/step - loss: 0.4015 - acc: 0.8872 - val_loss: 0.4421 - val_acc: 0.8182\n",
            "Epoch 12/20\n",
            "585/585 [==============================] - 1s 937us/step - loss: 0.3830 - acc: 0.8872 - val_loss: 0.3915 - val_acc: 0.8788\n",
            "Epoch 13/20\n",
            "585/585 [==============================] - 1s 894us/step - loss: 0.3614 - acc: 0.8940 - val_loss: 0.3683 - val_acc: 0.9394\n",
            "Epoch 14/20\n",
            "585/585 [==============================] - 1s 908us/step - loss: 0.3439 - acc: 0.8974 - val_loss: 0.3524 - val_acc: 0.9394\n",
            "Epoch 15/20\n",
            "585/585 [==============================] - 1s 911us/step - loss: 0.3309 - acc: 0.9111 - val_loss: 0.3491 - val_acc: 0.9091\n",
            "Epoch 16/20\n",
            "585/585 [==============================] - 1s 911us/step - loss: 0.3150 - acc: 0.9145 - val_loss: 0.3268 - val_acc: 0.9394\n",
            "Epoch 17/20\n",
            "585/585 [==============================] - 1s 894us/step - loss: 0.3048 - acc: 0.9162 - val_loss: 0.3219 - val_acc: 0.9091\n",
            "Epoch 18/20\n",
            "585/585 [==============================] - 1s 907us/step - loss: 0.2912 - acc: 0.9162 - val_loss: 0.3383 - val_acc: 0.8636\n",
            "Epoch 19/20\n",
            "585/585 [==============================] - 1s 876us/step - loss: 0.2810 - acc: 0.9265 - val_loss: 0.3265 - val_acc: 0.8788\n",
            "Epoch 20/20\n",
            "585/585 [==============================] - 1s 922us/step - loss: 0.2724 - acc: 0.9299 - val_loss: 0.2987 - val_acc: 0.8939\n",
            "163/163 [==============================] - 0s 122us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "17:31:33 WORKER: done with job (5, 0, 0), trying to register it.\n",
            "17:31:33 WORKER: registered result for job (5, 0, 0) with dispatcher\n",
            "17:31:33 DISPATCHER: job (5, 0, 0) finished\n",
            "17:31:33 DISPATCHER: register_result: lock acquired\n",
            "17:31:33 DISPATCHER: job (5, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368 finished\n",
            "17:31:33 job_id: (5, 0, 0)\n",
            "kwargs: {'config': {'activation': 'tanh', 'batch_size': 5, 'dense_units': 233, 'lr': 4.824982802774297e-05, 'number_of_layers': 0, 'optimizer': 'adam'}, 'budget': 20.0, 'working_directory': '.'}\n",
            "result: {'loss': 0.2987150290247166, 'info': {'dataset_used': 'Proteomes', 'test_accuracy': [0.3562021051630652, 0.8588957055214724]}}\n",
            "exception: None\n",
            "\n",
            "17:31:33 job_callback for (5, 0, 0) started\n",
            "17:31:33 job_callback for (5, 0, 0) got condition\n",
            "17:31:33 DISPATCHER: Trying to submit another job.\n",
            "17:31:33 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
            "17:31:33 Only 6 run(s) for budget 20.000000 available, need more than 9 -> can't build model!\n",
            "17:31:33 HBMASTER: Trying to run another job!\n",
            "17:31:33 job_callback for (5, 0, 0) finished\n",
            "17:31:33 start sampling a new configuration.\n",
            "17:31:33 done sampling a new configuration.\n",
            "17:31:33 HBMASTER: schedule new run for iteration 6\n",
            "17:31:33 HBMASTER: trying submitting job (6, 0, 0) to dispatcher\n",
            "17:31:33 HBMASTER: submitting job (6, 0, 0) to dispatcher\n",
            "17:31:33 DISPATCHER: trying to submit job (6, 0, 0)\n",
            "17:31:33 DISPATCHER: trying to notify the job_runner thread.\n",
            "17:31:33 HBMASTER: job (6, 0, 0) submitted to dispatcher\n",
            "17:31:33 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
            "17:31:33 DISPATCHER: Trying to submit another job.\n",
            "17:31:33 DISPATCHER: starting job (6, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:31:33 DISPATCHER: job (6, 0, 0) dispatched on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:31:33 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
            "17:31:33 WORKER: start processing job (6, 0, 0)\n",
            "17:31:33 WORKER: args: ()\n",
            "17:31:33 WORKER: kwargs: {'config': {'activation': 'tanh', 'batch_size': 4, 'dense_units': 78, 'lr': 0.0002389370532678974, 'number_of_layers': 2, 'optimizer': 'adam'}, 'budget': 20.0, 'working_directory': '.'}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 585 samples, validate on 66 samples\n",
            "Epoch 1/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.0403 - acc: 0.6120 - val_loss: 0.7600 - val_acc: 0.7424\n",
            "Epoch 2/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.5705 - acc: 0.8171 - val_loss: 0.4947 - val_acc: 0.8485\n",
            "Epoch 3/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.4002 - acc: 0.8598 - val_loss: 0.4050 - val_acc: 0.8636\n",
            "Epoch 4/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.3297 - acc: 0.8838 - val_loss: 0.3910 - val_acc: 0.8333\n",
            "Epoch 5/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.2831 - acc: 0.9026 - val_loss: 0.2790 - val_acc: 0.9545\n",
            "Epoch 6/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.2238 - acc: 0.9248 - val_loss: 0.4199 - val_acc: 0.8030\n",
            "Epoch 7/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.2112 - acc: 0.9248 - val_loss: 0.2330 - val_acc: 0.9091\n",
            "Epoch 8/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.1728 - acc: 0.9350 - val_loss: 0.2468 - val_acc: 0.9091\n",
            "Epoch 9/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.1750 - acc: 0.9350 - val_loss: 0.2654 - val_acc: 0.9091\n",
            "Epoch 10/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.1458 - acc: 0.9504 - val_loss: 0.2184 - val_acc: 0.9091\n",
            "Epoch 11/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.1247 - acc: 0.9590 - val_loss: 0.1929 - val_acc: 0.9091\n",
            "Epoch 12/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.1293 - acc: 0.9538 - val_loss: 0.2199 - val_acc: 0.9091\n",
            "Epoch 13/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.1209 - acc: 0.9590 - val_loss: 0.1582 - val_acc: 0.9091\n",
            "Epoch 14/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.0905 - acc: 0.9778 - val_loss: 0.2150 - val_acc: 0.9091\n",
            "Epoch 15/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.0811 - acc: 0.9778 - val_loss: 0.2299 - val_acc: 0.9091\n",
            "Epoch 16/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.0737 - acc: 0.9812 - val_loss: 0.4579 - val_acc: 0.8333\n",
            "Epoch 17/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.0751 - acc: 0.9795 - val_loss: 0.2006 - val_acc: 0.9242\n",
            "Epoch 18/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.0578 - acc: 0.9949 - val_loss: 0.2718 - val_acc: 0.9091\n",
            "Epoch 19/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.0591 - acc: 0.9812 - val_loss: 0.2702 - val_acc: 0.9091\n",
            "Epoch 20/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.0464 - acc: 0.9880 - val_loss: 0.1674 - val_acc: 0.9242\n",
            "163/163 [==============================] - 0s 98us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "17:31:50 WORKER: done with job (6, 0, 0), trying to register it.\n",
            "17:31:50 WORKER: registered result for job (6, 0, 0) with dispatcher\n",
            "17:31:50 DISPATCHER: job (6, 0, 0) finished\n",
            "17:31:50 DISPATCHER: register_result: lock acquired\n",
            "17:31:50 DISPATCHER: job (6, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368 finished\n",
            "17:31:50 job_id: (6, 0, 0)\n",
            "kwargs: {'config': {'activation': 'tanh', 'batch_size': 4, 'dense_units': 78, 'lr': 0.0002389370532678974, 'number_of_layers': 2, 'optimizer': 'adam'}, 'budget': 20.0, 'working_directory': '.'}\n",
            "result: {'loss': 0.16737410759158206, 'info': {'dataset_used': 'Proteomes', 'test_accuracy': [0.34196583105056927, 0.8895705521472392]}}\n",
            "exception: None\n",
            "\n",
            "17:31:50 job_callback for (6, 0, 0) started\n",
            "17:31:50 DISPATCHER: Trying to submit another job.\n",
            "17:31:50 job_callback for (6, 0, 0) got condition\n",
            "17:31:50 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
            "17:31:50 Only 7 run(s) for budget 20.000000 available, need more than 9 -> can't build model!\n",
            "17:31:50 HBMASTER: Trying to run another job!\n",
            "17:31:50 job_callback for (6, 0, 0) finished\n",
            "17:31:50 start sampling a new configuration.\n",
            "17:31:50 done sampling a new configuration.\n",
            "17:31:50 HBMASTER: schedule new run for iteration 7\n",
            "17:31:50 HBMASTER: trying submitting job (7, 0, 0) to dispatcher\n",
            "17:31:50 HBMASTER: submitting job (7, 0, 0) to dispatcher\n",
            "17:31:50 DISPATCHER: trying to submit job (7, 0, 0)\n",
            "17:31:50 DISPATCHER: trying to notify the job_runner thread.\n",
            "17:31:50 HBMASTER: job (7, 0, 0) submitted to dispatcher\n",
            "17:31:50 DISPATCHER: Trying to submit another job.\n",
            "17:31:50 DISPATCHER: starting job (7, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:31:50 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
            "17:31:50 DISPATCHER: job (7, 0, 0) dispatched on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:31:50 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
            "17:31:50 WORKER: start processing job (7, 0, 0)\n",
            "17:31:50 WORKER: args: ()\n",
            "17:31:50 WORKER: kwargs: {'config': {'activation': 'tanh', 'batch_size': 2, 'dense_units': 82, 'lr': 2.4549909280703935e-05, 'number_of_layers': 2, 'optimizer': 'Nadam'}, 'budget': 20.0, 'working_directory': '.'}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 585 samples, validate on 66 samples\n",
            "Epoch 1/20\n",
            "585/585 [==============================] - 2s 4ms/step - loss: 1.5092 - acc: 0.3214 - val_loss: 1.3984 - val_acc: 0.3333\n",
            "Epoch 2/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 1.2234 - acc: 0.4735 - val_loss: 1.2334 - val_acc: 0.4394\n",
            "Epoch 3/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 1.0700 - acc: 0.5778 - val_loss: 1.0796 - val_acc: 0.5606\n",
            "Epoch 4/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 0.9294 - acc: 0.6667 - val_loss: 0.9521 - val_acc: 0.6061\n",
            "Epoch 5/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 0.8163 - acc: 0.7060 - val_loss: 0.8304 - val_acc: 0.7121\n",
            "Epoch 6/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 0.7262 - acc: 0.7556 - val_loss: 0.7590 - val_acc: 0.6667\n",
            "Epoch 7/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 0.6557 - acc: 0.7915 - val_loss: 0.6739 - val_acc: 0.8030\n",
            "Epoch 8/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 0.5988 - acc: 0.7949 - val_loss: 0.6267 - val_acc: 0.7576\n",
            "Epoch 9/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 0.5488 - acc: 0.8239 - val_loss: 0.5773 - val_acc: 0.7727\n",
            "Epoch 10/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 0.5070 - acc: 0.8427 - val_loss: 0.5268 - val_acc: 0.8333\n",
            "Epoch 11/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 0.4703 - acc: 0.8581 - val_loss: 0.4856 - val_acc: 0.8333\n",
            "Epoch 12/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 0.4407 - acc: 0.8632 - val_loss: 0.4509 - val_acc: 0.8333\n",
            "Epoch 13/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 0.4137 - acc: 0.8735 - val_loss: 0.4294 - val_acc: 0.8636\n",
            "Epoch 14/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 0.3900 - acc: 0.8889 - val_loss: 0.4034 - val_acc: 0.8636\n",
            "Epoch 15/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 0.3697 - acc: 0.8923 - val_loss: 0.3809 - val_acc: 0.8636\n",
            "Epoch 16/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 0.3488 - acc: 0.9009 - val_loss: 0.3700 - val_acc: 0.8788\n",
            "Epoch 17/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 0.3345 - acc: 0.8957 - val_loss: 0.3569 - val_acc: 0.8788\n",
            "Epoch 18/20\n",
            " 20/585 [>.............................] - ETA: 1s - loss: 0.2287 - acc: 0.9000"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "17:32:21 DISPATCHER: Starting worker discovery\n",
            "17:32:21 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
            "17:32:21 DISPATCHER: Finished worker discovery\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "585/585 [==============================] - 2s 3ms/step - loss: 0.3195 - acc: 0.9077 - val_loss: 0.3477 - val_acc: 0.8939\n",
            "Epoch 19/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 0.3045 - acc: 0.9111 - val_loss: 0.3421 - val_acc: 0.9091\n",
            "Epoch 20/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 0.2910 - acc: 0.9094 - val_loss: 0.3299 - val_acc: 0.9091\n",
            "163/163 [==============================] - 0s 97us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "17:32:26 WORKER: done with job (7, 0, 0), trying to register it.\n",
            "17:32:26 DISPATCHER: job (7, 0, 0) finished\n",
            "17:32:26 WORKER: registered result for job (7, 0, 0) with dispatcher\n",
            "17:32:26 DISPATCHER: register_result: lock acquired\n",
            "17:32:26 DISPATCHER: job (7, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368 finished\n",
            "17:32:26 job_id: (7, 0, 0)\n",
            "kwargs: {'config': {'activation': 'tanh', 'batch_size': 2, 'dense_units': 82, 'lr': 2.4549909280703935e-05, 'number_of_layers': 2, 'optimizer': 'Nadam'}, 'budget': 20.0, 'working_directory': '.'}\n",
            "result: {'loss': 0.3299054452077006, 'info': {'dataset_used': 'Proteomes', 'test_accuracy': [0.3724433759604495, 0.8773006134969326]}}\n",
            "exception: None\n",
            "\n",
            "17:32:26 job_callback for (7, 0, 0) started\n",
            "17:32:26 DISPATCHER: Trying to submit another job.\n",
            "17:32:26 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
            "17:32:26 job_callback for (7, 0, 0) got condition\n",
            "17:32:26 HBMASTER: Trying to run another job!\n",
            "17:32:26 job_callback for (7, 0, 0) finished\n",
            "17:32:26 start sampling a new configuration.\n",
            "17:32:26 done sampling a new configuration.\n",
            "17:32:26 HBMASTER: schedule new run for iteration 8\n",
            "17:32:26 HBMASTER: trying submitting job (8, 0, 0) to dispatcher\n",
            "17:32:26 HBMASTER: submitting job (8, 0, 0) to dispatcher\n",
            "17:32:26 DISPATCHER: trying to submit job (8, 0, 0)\n",
            "17:32:26 DISPATCHER: trying to notify the job_runner thread.\n",
            "17:32:26 HBMASTER: job (8, 0, 0) submitted to dispatcher\n",
            "17:32:26 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
            "17:32:26 DISPATCHER: Trying to submit another job.\n",
            "17:32:26 DISPATCHER: starting job (8, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:32:26 DISPATCHER: job (8, 0, 0) dispatched on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:32:26 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
            "17:32:26 WORKER: start processing job (8, 0, 0)\n",
            "17:32:26 WORKER: args: ()\n",
            "17:32:26 WORKER: kwargs: {'config': {'activation': 'relu', 'batch_size': 5, 'dense_units': 158, 'lr': 0.004756138648703685, 'number_of_layers': 4, 'optimizer': 'Nadam'}, 'budget': 20.0, 'working_directory': '.'}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 585 samples, validate on 66 samples\n",
            "Epoch 1/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.1971 - acc: 0.5641 - val_loss: 1.1004 - val_acc: 0.6212\n",
            "Epoch 2/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.7502 - acc: 0.7402 - val_loss: 0.7700 - val_acc: 0.6515\n",
            "Epoch 3/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.5668 - acc: 0.8017 - val_loss: 0.7758 - val_acc: 0.7273\n",
            "Epoch 4/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.5780 - acc: 0.8068 - val_loss: 0.4185 - val_acc: 0.8485\n",
            "Epoch 5/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.4212 - acc: 0.8684 - val_loss: 0.4117 - val_acc: 0.8788\n",
            "Epoch 6/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.4617 - acc: 0.8530 - val_loss: 0.7854 - val_acc: 0.5758\n",
            "Epoch 7/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.3918 - acc: 0.8667 - val_loss: 0.5092 - val_acc: 0.8788\n",
            "Epoch 8/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.4026 - acc: 0.8821 - val_loss: 0.3850 - val_acc: 0.8939\n",
            "Epoch 9/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.4122 - acc: 0.8667 - val_loss: 0.2892 - val_acc: 0.8788\n",
            "Epoch 10/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.3876 - acc: 0.8769 - val_loss: 0.4193 - val_acc: 0.8182\n",
            "Epoch 11/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.3908 - acc: 0.8940 - val_loss: 1.0772 - val_acc: 0.6970\n",
            "Epoch 12/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.4161 - acc: 0.8752 - val_loss: 1.1033 - val_acc: 0.6970\n",
            "Epoch 13/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.2639 - acc: 0.9077 - val_loss: 0.4016 - val_acc: 0.8485\n",
            "Epoch 14/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.2176 - acc: 0.9231 - val_loss: 0.3267 - val_acc: 0.8333\n",
            "Epoch 15/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.2423 - acc: 0.9368 - val_loss: 0.4694 - val_acc: 0.8788\n",
            "Epoch 16/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.3270 - acc: 0.9162 - val_loss: 0.6132 - val_acc: 0.8182\n",
            "Epoch 17/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.6067 - acc: 0.8359 - val_loss: 0.6371 - val_acc: 0.7879\n",
            "Epoch 18/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.5841 - acc: 0.8325 - val_loss: 0.4305 - val_acc: 0.8636\n",
            "Epoch 19/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.4277 - acc: 0.8188 - val_loss: 2.7569 - val_acc: 0.7727\n",
            "Epoch 20/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.3992 - acc: 0.8154 - val_loss: 0.6792 - val_acc: 0.6667\n",
            "163/163 [==============================] - 0s 106us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "17:32:46 WORKER: done with job (8, 0, 0), trying to register it.\n",
            "17:32:46 WORKER: registered result for job (8, 0, 0) with dispatcher\n",
            "17:32:46 DISPATCHER: job (8, 0, 0) finished\n",
            "17:32:46 DISPATCHER: register_result: lock acquired\n",
            "17:32:46 DISPATCHER: job (8, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368 finished\n",
            "17:32:46 job_id: (8, 0, 0)\n",
            "kwargs: {'config': {'activation': 'relu', 'batch_size': 5, 'dense_units': 158, 'lr': 0.004756138648703685, 'number_of_layers': 4, 'optimizer': 'Nadam'}, 'budget': 20.0, 'working_directory': '.'}\n",
            "result: {'loss': 0.6792212379249659, 'info': {'dataset_used': 'Proteomes', 'test_accuracy': [0.7342855484207715, 0.6748466261325439]}}\n",
            "exception: None\n",
            "\n",
            "17:32:46 job_callback for (8, 0, 0) started\n",
            "17:32:46 DISPATCHER: Trying to submit another job.\n",
            "17:32:46 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
            "17:32:46 job_callback for (8, 0, 0) got condition\n",
            "17:32:46 HBMASTER: Trying to run another job!\n",
            "17:32:46 job_callback for (8, 0, 0) finished\n",
            "17:32:46 start sampling a new configuration.\n",
            "17:32:46 done sampling a new configuration.\n",
            "17:32:46 HBMASTER: schedule new run for iteration 9\n",
            "17:32:46 HBMASTER: trying submitting job (9, 0, 0) to dispatcher\n",
            "17:32:46 HBMASTER: submitting job (9, 0, 0) to dispatcher\n",
            "17:32:46 DISPATCHER: trying to submit job (9, 0, 0)\n",
            "17:32:46 DISPATCHER: trying to notify the job_runner thread.\n",
            "17:32:46 HBMASTER: job (9, 0, 0) submitted to dispatcher\n",
            "17:32:46 DISPATCHER: Trying to submit another job.\n",
            "17:32:46 DISPATCHER: starting job (9, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:32:46 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
            "17:32:46 DISPATCHER: job (9, 0, 0) dispatched on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:32:46 WORKER: start processing job (9, 0, 0)\n",
            "17:32:46 WORKER: args: ()\n",
            "17:32:46 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
            "17:32:46 WORKER: kwargs: {'config': {'activation': 'tanh', 'batch_size': 5, 'dense_units': 74, 'lr': 8.025981441119337e-06, 'number_of_layers': 2, 'optimizer': 'adam'}, 'budget': 20.0, 'working_directory': '.'}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 585 samples, validate on 66 samples\n",
            "Epoch 1/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.4841 - acc: 0.2462 - val_loss: 1.4923 - val_acc: 0.2879\n",
            "Epoch 2/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 1.4140 - acc: 0.3385 - val_loss: 1.4559 - val_acc: 0.3030\n",
            "Epoch 3/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 1.3658 - acc: 0.3795 - val_loss: 1.4271 - val_acc: 0.3182\n",
            "Epoch 4/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 1.3288 - acc: 0.4154 - val_loss: 1.4034 - val_acc: 0.3333\n",
            "Epoch 5/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 1.2986 - acc: 0.4530 - val_loss: 1.3808 - val_acc: 0.3333\n",
            "Epoch 6/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 1.2715 - acc: 0.4701 - val_loss: 1.3592 - val_acc: 0.3636\n",
            "Epoch 7/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 1.2471 - acc: 0.5231 - val_loss: 1.3373 - val_acc: 0.3788\n",
            "Epoch 8/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 1.2244 - acc: 0.5350 - val_loss: 1.3156 - val_acc: 0.3939\n",
            "Epoch 9/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 1.2018 - acc: 0.5402 - val_loss: 1.2918 - val_acc: 0.4394\n",
            "Epoch 10/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 1.1802 - acc: 0.5573 - val_loss: 1.2725 - val_acc: 0.4394\n",
            "Epoch 11/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 1.1591 - acc: 0.5761 - val_loss: 1.2511 - val_acc: 0.4545\n",
            "Epoch 12/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 1.1383 - acc: 0.5932 - val_loss: 1.2298 - val_acc: 0.4848\n",
            "Epoch 13/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 1.1175 - acc: 0.6034 - val_loss: 1.2128 - val_acc: 0.4848\n",
            "Epoch 14/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 1.0966 - acc: 0.6068 - val_loss: 1.1922 - val_acc: 0.5000\n",
            "Epoch 15/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 1.0763 - acc: 0.6256 - val_loss: 1.1691 - val_acc: 0.5152\n",
            "Epoch 16/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 1.0559 - acc: 0.6376 - val_loss: 1.1497 - val_acc: 0.5152\n",
            "Epoch 17/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 1.0355 - acc: 0.6598 - val_loss: 1.1255 - val_acc: 0.5303\n",
            "Epoch 18/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 1.0151 - acc: 0.6632 - val_loss: 1.1075 - val_acc: 0.5152\n",
            "Epoch 19/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.9951 - acc: 0.6718 - val_loss: 1.0890 - val_acc: 0.5152\n",
            "Epoch 20/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.9756 - acc: 0.6821 - val_loss: 1.0690 - val_acc: 0.5303\n",
            "163/163 [==============================] - 0s 101us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "17:32:59 WORKER: done with job (9, 0, 0), trying to register it.\n",
            "17:32:59 WORKER: registered result for job (9, 0, 0) with dispatcher\n",
            "17:32:59 DISPATCHER: job (9, 0, 0) finished\n",
            "17:32:59 DISPATCHER: register_result: lock acquired\n",
            "17:32:59 DISPATCHER: job (9, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368 finished\n",
            "17:32:59 job_id: (9, 0, 0)\n",
            "kwargs: {'config': {'activation': 'tanh', 'batch_size': 5, 'dense_units': 74, 'lr': 8.025981441119337e-06, 'number_of_layers': 2, 'optimizer': 'adam'}, 'budget': 20.0, 'working_directory': '.'}\n",
            "result: {'loss': 1.0690252582232158, 'info': {'dataset_used': 'Proteomes', 'test_accuracy': [0.9464898160630209, 0.6871165646000142]}}\n",
            "exception: None\n",
            "\n",
            "17:32:59 job_callback for (9, 0, 0) started\n",
            "17:32:59 DISPATCHER: Trying to submit another job.\n",
            "17:32:59 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
            "17:32:59 job_callback for (9, 0, 0) got condition\n",
            "17:32:59 HBMASTER: Trying to run another job!\n",
            "17:32:59 job_callback for (9, 0, 0) finished\n",
            "17:32:59 start sampling a new configuration.\n",
            "17:32:59 done sampling a new configuration.\n",
            "17:32:59 HBMASTER: schedule new run for iteration 10\n",
            "17:32:59 HBMASTER: trying submitting job (10, 0, 0) to dispatcher\n",
            "17:32:59 HBMASTER: submitting job (10, 0, 0) to dispatcher\n",
            "17:32:59 DISPATCHER: trying to submit job (10, 0, 0)\n",
            "17:32:59 DISPATCHER: trying to notify the job_runner thread.\n",
            "17:32:59 HBMASTER: job (10, 0, 0) submitted to dispatcher\n",
            "17:32:59 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
            "17:32:59 DISPATCHER: Trying to submit another job.\n",
            "17:32:59 DISPATCHER: starting job (10, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:32:59 DISPATCHER: job (10, 0, 0) dispatched on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:32:59 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
            "17:32:59 WORKER: start processing job (10, 0, 0)\n",
            "17:32:59 WORKER: args: ()\n",
            "17:32:59 WORKER: kwargs: {'config': {'activation': 'tanh', 'batch_size': 4, 'dense_units': 109, 'lr': 0.00586244567127264, 'number_of_layers': 3, 'optimizer': 'SGD', 'sgd_momentum': 0.1280186115821687}, 'budget': 20.0, 'working_directory': '.'}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 585 samples, validate on 66 samples\n",
            "Epoch 1/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.1111 - acc: 0.5470 - val_loss: 1.4180 - val_acc: 0.4394\n",
            "Epoch 2/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.7518 - acc: 0.7060 - val_loss: 0.9675 - val_acc: 0.5909\n",
            "Epoch 3/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.6175 - acc: 0.7624 - val_loss: 2.2627 - val_acc: 0.3030\n",
            "Epoch 4/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.6529 - acc: 0.7402 - val_loss: 0.5949 - val_acc: 0.7273\n",
            "Epoch 5/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.5323 - acc: 0.8034 - val_loss: 0.7605 - val_acc: 0.6667\n",
            "Epoch 6/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.5003 - acc: 0.7966 - val_loss: 0.5687 - val_acc: 0.7727\n",
            "Epoch 7/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.4834 - acc: 0.7915 - val_loss: 0.4085 - val_acc: 0.8030\n",
            "Epoch 8/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.4143 - acc: 0.8410 - val_loss: 0.7439 - val_acc: 0.7273\n",
            "Epoch 9/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.4772 - acc: 0.8034 - val_loss: 0.8028 - val_acc: 0.6818\n",
            "Epoch 10/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.4151 - acc: 0.8444 - val_loss: 0.6755 - val_acc: 0.6818\n",
            "Epoch 11/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.4203 - acc: 0.8376 - val_loss: 0.5006 - val_acc: 0.7727\n",
            "Epoch 12/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.4246 - acc: 0.8342 - val_loss: 0.4788 - val_acc: 0.8182\n",
            "Epoch 13/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.4250 - acc: 0.8256 - val_loss: 0.4391 - val_acc: 0.8788\n",
            "Epoch 14/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.3888 - acc: 0.8530 - val_loss: 1.0573 - val_acc: 0.6364\n",
            "Epoch 15/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.4508 - acc: 0.8342 - val_loss: 0.7959 - val_acc: 0.6667\n",
            "Epoch 16/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.4089 - acc: 0.8513 - val_loss: 1.0668 - val_acc: 0.5606\n",
            "Epoch 17/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.3947 - acc: 0.8462 - val_loss: 0.7229 - val_acc: 0.6515\n",
            "Epoch 18/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.3728 - acc: 0.8462 - val_loss: 0.3344 - val_acc: 0.8636\n",
            "Epoch 19/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.3995 - acc: 0.8530 - val_loss: 0.5122 - val_acc: 0.8030\n",
            "Epoch 20/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 0.3885 - acc: 0.8462 - val_loss: 0.4880 - val_acc: 0.8182\n",
            "163/163 [==============================] - 0s 103us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "17:33:14 WORKER: done with job (10, 0, 0), trying to register it.\n",
            "17:33:14 WORKER: registered result for job (10, 0, 0) with dispatcher\n",
            "17:33:14 DISPATCHER: job (10, 0, 0) finished\n",
            "17:33:14 DISPATCHER: register_result: lock acquired\n",
            "17:33:14 DISPATCHER: job (10, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368 finished\n",
            "17:33:14 job_id: (10, 0, 0)\n",
            "kwargs: {'config': {'activation': 'tanh', 'batch_size': 4, 'dense_units': 109, 'lr': 0.00586244567127264, 'number_of_layers': 3, 'optimizer': 'SGD', 'sgd_momentum': 0.1280186115821687}, 'budget': 20.0, 'working_directory': '.'}\n",
            "result: {'loss': 0.48800100363565213, 'info': {'dataset_used': 'Proteomes', 'test_accuracy': [0.4925865807050576, 0.7852760736196319]}}\n",
            "exception: None\n",
            "\n",
            "17:33:14 job_callback for (10, 0, 0) started\n",
            "17:33:14 DISPATCHER: Trying to submit another job.\n",
            "17:33:14 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
            "17:33:14 job_callback for (10, 0, 0) got condition\n",
            "17:33:14 HBMASTER: Trying to run another job!\n",
            "17:33:14 job_callback for (10, 0, 0) finished\n",
            "17:33:14 start sampling a new configuration.\n",
            "17:33:14 done sampling a new configuration.\n",
            "17:33:14 HBMASTER: schedule new run for iteration 11\n",
            "17:33:14 HBMASTER: trying submitting job (11, 0, 0) to dispatcher\n",
            "17:33:14 HBMASTER: submitting job (11, 0, 0) to dispatcher\n",
            "17:33:14 DISPATCHER: trying to submit job (11, 0, 0)\n",
            "17:33:14 DISPATCHER: trying to notify the job_runner thread.\n",
            "17:33:14 HBMASTER: job (11, 0, 0) submitted to dispatcher\n",
            "17:33:14 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
            "17:33:14 DISPATCHER: Trying to submit another job.\n",
            "17:33:14 DISPATCHER: starting job (11, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:33:14 DISPATCHER: job (11, 0, 0) dispatched on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:33:14 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
            "17:33:14 WORKER: start processing job (11, 0, 0)\n",
            "17:33:14 WORKER: args: ()\n",
            "17:33:14 WORKER: kwargs: {'config': {'activation': 'relu', 'batch_size': 3, 'dense_units': 118, 'lr': 0.0006453853848492256, 'number_of_layers': 1, 'optimizer': 'Nadam'}, 'budget': 20.0, 'working_directory': '.'}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 585 samples, validate on 66 samples\n",
            "Epoch 1/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.7727 - acc: 0.7333 - val_loss: 0.4540 - val_acc: 0.7727\n",
            "Epoch 2/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.3715 - acc: 0.8650 - val_loss: 0.5203 - val_acc: 0.7576\n",
            "Epoch 3/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.2599 - acc: 0.9077 - val_loss: 0.3369 - val_acc: 0.8788\n",
            "Epoch 4/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.2357 - acc: 0.9094 - val_loss: 0.3085 - val_acc: 0.8485\n",
            "Epoch 5/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.2165 - acc: 0.9060 - val_loss: 0.4287 - val_acc: 0.7879\n",
            "Epoch 6/20\n",
            "516/585 [=========================>....] - ETA: 0s - loss: 0.1804 - acc: 0.9244"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "17:33:21 DISPATCHER: Starting worker discovery\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r546/585 [===========================>..] - ETA: 0s - loss: 0.1782 - acc: 0.9267"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "17:33:21 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
            "17:33:21 DISPATCHER: Finished worker discovery\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "585/585 [==============================] - 1s 2ms/step - loss: 0.1788 - acc: 0.9282 - val_loss: 0.4348 - val_acc: 0.7879\n",
            "Epoch 7/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.1564 - acc: 0.9436 - val_loss: 0.3631 - val_acc: 0.8788\n",
            "Epoch 8/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.1653 - acc: 0.9385 - val_loss: 0.3507 - val_acc: 0.8636\n",
            "Epoch 9/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.1343 - acc: 0.9470 - val_loss: 0.3358 - val_acc: 0.8788\n",
            "Epoch 10/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.1496 - acc: 0.9419 - val_loss: 0.2852 - val_acc: 0.8788\n",
            "Epoch 11/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.1145 - acc: 0.9504 - val_loss: 0.2661 - val_acc: 0.8939\n",
            "Epoch 12/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.1149 - acc: 0.9521 - val_loss: 0.2264 - val_acc: 0.9091\n",
            "Epoch 13/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.1005 - acc: 0.9624 - val_loss: 0.1438 - val_acc: 0.9091\n",
            "Epoch 14/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.1088 - acc: 0.9504 - val_loss: 0.2580 - val_acc: 0.8939\n",
            "Epoch 15/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.0847 - acc: 0.9658 - val_loss: 0.2245 - val_acc: 0.8939\n",
            "Epoch 16/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.0674 - acc: 0.9761 - val_loss: 0.4512 - val_acc: 0.8333\n",
            "Epoch 17/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.1086 - acc: 0.9590 - val_loss: 0.1494 - val_acc: 0.9091\n",
            "Epoch 18/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.0656 - acc: 0.9744 - val_loss: 0.1966 - val_acc: 0.8939\n",
            "Epoch 19/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.0604 - acc: 0.9761 - val_loss: 0.1142 - val_acc: 0.9545\n",
            "Epoch 20/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.0696 - acc: 0.9761 - val_loss: 0.3050 - val_acc: 0.8636\n",
            "163/163 [==============================] - 0s 111us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "17:33:36 WORKER: done with job (11, 0, 0), trying to register it.\n",
            "17:33:36 WORKER: registered result for job (11, 0, 0) with dispatcher\n",
            "17:33:36 DISPATCHER: job (11, 0, 0) finished\n",
            "17:33:36 DISPATCHER: register_result: lock acquired\n",
            "17:33:36 DISPATCHER: job (11, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368 finished\n",
            "17:33:36 job_id: (11, 0, 0)\n",
            "kwargs: {'config': {'activation': 'relu', 'batch_size': 3, 'dense_units': 118, 'lr': 0.0006453853848492256, 'number_of_layers': 1, 'optimizer': 'Nadam'}, 'budget': 20.0, 'working_directory': '.'}\n",
            "result: {'loss': 0.3050251070023726, 'info': {'dataset_used': 'Proteomes', 'test_accuracy': [0.3814021332012318, 0.9079754601226994]}}\n",
            "exception: None\n",
            "\n",
            "17:33:36 job_callback for (11, 0, 0) started\n",
            "17:33:36 DISPATCHER: Trying to submit another job.\n",
            "17:33:36 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
            "17:33:36 job_callback for (11, 0, 0) got condition\n",
            "17:33:36 HBMASTER: Trying to run another job!\n",
            "17:33:36 job_callback for (11, 0, 0) finished\n",
            "17:33:36 start sampling a new configuration.\n",
            "17:33:36 done sampling a new configuration.\n",
            "17:33:36 HBMASTER: schedule new run for iteration 12\n",
            "17:33:36 HBMASTER: trying submitting job (12, 0, 0) to dispatcher\n",
            "17:33:36 HBMASTER: submitting job (12, 0, 0) to dispatcher\n",
            "17:33:36 DISPATCHER: trying to submit job (12, 0, 0)\n",
            "17:33:36 DISPATCHER: trying to notify the job_runner thread.\n",
            "17:33:36 HBMASTER: job (12, 0, 0) submitted to dispatcher\n",
            "17:33:36 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
            "17:33:36 DISPATCHER: Trying to submit another job.\n",
            "17:33:36 DISPATCHER: starting job (12, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:33:36 DISPATCHER: job (12, 0, 0) dispatched on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:33:36 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
            "17:33:36 WORKER: start processing job (12, 0, 0)\n",
            "17:33:36 WORKER: args: ()\n",
            "17:33:36 WORKER: kwargs: {'config': {'activation': 'tanh', 'batch_size': 2, 'dense_units': 223, 'lr': 6.904581065345013e-06, 'number_of_layers': 1, 'optimizer': 'SGD', 'sgd_momentum': 0.5364103893161447}, 'budget': 20.0, 'working_directory': '.'}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 585 samples, validate on 66 samples\n",
            "Epoch 1/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.6723 - acc: 0.2667 - val_loss: 1.6081 - val_acc: 0.2727\n",
            "Epoch 2/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.5513 - acc: 0.2496 - val_loss: 1.5377 - val_acc: 0.2879\n",
            "Epoch 3/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.4780 - acc: 0.2547 - val_loss: 1.4949 - val_acc: 0.3030\n",
            "Epoch 4/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.4282 - acc: 0.2718 - val_loss: 1.4638 - val_acc: 0.3182\n",
            "Epoch 5/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.3905 - acc: 0.3231 - val_loss: 1.4378 - val_acc: 0.3485\n",
            "Epoch 6/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.3596 - acc: 0.3726 - val_loss: 1.4150 - val_acc: 0.3939\n",
            "Epoch 7/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.3333 - acc: 0.3966 - val_loss: 1.3945 - val_acc: 0.3788\n",
            "Epoch 8/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.3101 - acc: 0.4359 - val_loss: 1.3760 - val_acc: 0.4091\n",
            "Epoch 9/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.2891 - acc: 0.4530 - val_loss: 1.3589 - val_acc: 0.3939\n",
            "Epoch 10/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.2704 - acc: 0.4701 - val_loss: 1.3429 - val_acc: 0.4091\n",
            "Epoch 11/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.2531 - acc: 0.4923 - val_loss: 1.3282 - val_acc: 0.4242\n",
            "Epoch 12/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.2371 - acc: 0.5111 - val_loss: 1.3146 - val_acc: 0.4091\n",
            "Epoch 13/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.2223 - acc: 0.5214 - val_loss: 1.3019 - val_acc: 0.4091\n",
            "Epoch 14/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.2083 - acc: 0.5197 - val_loss: 1.2895 - val_acc: 0.4091\n",
            "Epoch 15/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.1949 - acc: 0.5368 - val_loss: 1.2771 - val_acc: 0.4394\n",
            "Epoch 16/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.1826 - acc: 0.5402 - val_loss: 1.2652 - val_acc: 0.4242\n",
            "Epoch 17/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.1706 - acc: 0.5556 - val_loss: 1.2545 - val_acc: 0.4394\n",
            "Epoch 18/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.1591 - acc: 0.5692 - val_loss: 1.2435 - val_acc: 0.4697\n",
            "Epoch 19/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.1485 - acc: 0.5726 - val_loss: 1.2334 - val_acc: 0.4697\n",
            "Epoch 20/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.1379 - acc: 0.5846 - val_loss: 1.2251 - val_acc: 0.4545\n",
            "163/163 [==============================] - 0s 94us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "17:34:00 WORKER: done with job (12, 0, 0), trying to register it.\n",
            "17:34:00 WORKER: registered result for job (12, 0, 0) with dispatcher\n",
            "17:34:00 DISPATCHER: job (12, 0, 0) finished\n",
            "17:34:00 DISPATCHER: register_result: lock acquired\n",
            "17:34:00 DISPATCHER: job (12, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368 finished\n",
            "17:34:00 job_id: (12, 0, 0)\n",
            "kwargs: {'config': {'activation': 'tanh', 'batch_size': 2, 'dense_units': 223, 'lr': 6.904581065345013e-06, 'number_of_layers': 1, 'optimizer': 'SGD', 'sgd_momentum': 0.5364103893161447}, 'budget': 20.0, 'working_directory': '.'}\n",
            "result: {'loss': 1.2250547824483928, 'info': {'dataset_used': 'Proteomes', 'test_accuracy': [1.1073873613509664, 0.5889570553975603]}}\n",
            "exception: None\n",
            "\n",
            "17:34:00 job_callback for (12, 0, 0) started\n",
            "17:34:00 DISPATCHER: Trying to submit another job.\n",
            "17:34:00 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
            "17:34:00 job_callback for (12, 0, 0) got condition\n",
            "17:34:00 HBMASTER: Trying to run another job!\n",
            "17:34:00 job_callback for (12, 0, 0) finished\n",
            "17:34:00 start sampling a new configuration.\n",
            "17:34:00 done sampling a new configuration.\n",
            "17:34:00 HBMASTER: schedule new run for iteration 13\n",
            "17:34:00 HBMASTER: trying submitting job (13, 0, 0) to dispatcher\n",
            "17:34:00 HBMASTER: submitting job (13, 0, 0) to dispatcher\n",
            "17:34:00 DISPATCHER: trying to submit job (13, 0, 0)\n",
            "17:34:00 DISPATCHER: trying to notify the job_runner thread.\n",
            "17:34:00 HBMASTER: job (13, 0, 0) submitted to dispatcher\n",
            "17:34:00 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
            "17:34:00 DISPATCHER: Trying to submit another job.\n",
            "17:34:00 DISPATCHER: starting job (13, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:34:00 DISPATCHER: job (13, 0, 0) dispatched on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:34:00 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
            "17:34:00 WORKER: start processing job (13, 0, 0)\n",
            "17:34:00 WORKER: args: ()\n",
            "17:34:00 WORKER: kwargs: {'config': {'activation': 'relu', 'batch_size': 3, 'dense_units': 114, 'lr': 0.00019062182011928854, 'number_of_layers': 4, 'optimizer': 'adam'}, 'budget': 20.0, 'working_directory': '.'}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 585 samples, validate on 66 samples\n",
            "Epoch 1/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 1.0216 - acc: 0.6068 - val_loss: 0.7015 - val_acc: 0.7576\n",
            "Epoch 2/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.5596 - acc: 0.7556 - val_loss: 0.4497 - val_acc: 0.8636\n",
            "Epoch 3/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.3737 - acc: 0.8462 - val_loss: 0.4898 - val_acc: 0.8182\n",
            "Epoch 4/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.3346 - acc: 0.8547 - val_loss: 0.3757 - val_acc: 0.8333\n",
            "Epoch 5/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.2615 - acc: 0.8906 - val_loss: 0.2924 - val_acc: 0.9091\n",
            "Epoch 6/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.2254 - acc: 0.8991 - val_loss: 0.2936 - val_acc: 0.8788\n",
            "Epoch 7/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.1849 - acc: 0.9248 - val_loss: 0.4136 - val_acc: 0.8333\n",
            "Epoch 8/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.1741 - acc: 0.9248 - val_loss: 0.3458 - val_acc: 0.8788\n",
            "Epoch 9/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.1770 - acc: 0.9333 - val_loss: 0.4909 - val_acc: 0.8333\n",
            "Epoch 10/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.1719 - acc: 0.9333 - val_loss: 0.2705 - val_acc: 0.8939\n",
            "Epoch 11/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.1634 - acc: 0.9368 - val_loss: 0.4286 - val_acc: 0.8485\n",
            "Epoch 12/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.1292 - acc: 0.9556 - val_loss: 0.2425 - val_acc: 0.9394\n",
            "Epoch 13/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.1205 - acc: 0.9573 - val_loss: 0.4346 - val_acc: 0.8485\n",
            "Epoch 14/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.1165 - acc: 0.9385 - val_loss: 0.3156 - val_acc: 0.8636\n",
            "Epoch 15/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.1103 - acc: 0.9590 - val_loss: 0.2005 - val_acc: 0.9242\n",
            "Epoch 16/20\n",
            "318/585 [===============>..............] - ETA: 0s - loss: 0.1033 - acc: 0.9591"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "17:34:21 DISPATCHER: Starting worker discovery\n",
            "17:34:21 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
            "17:34:21 DISPATCHER: Finished worker discovery\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "585/585 [==============================] - 1s 2ms/step - loss: 0.1033 - acc: 0.9658 - val_loss: 0.2425 - val_acc: 0.8939\n",
            "Epoch 17/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.0747 - acc: 0.9778 - val_loss: 0.4268 - val_acc: 0.9091\n",
            "Epoch 18/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.0976 - acc: 0.9692 - val_loss: 0.2359 - val_acc: 0.8939\n",
            "Epoch 19/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.0832 - acc: 0.9761 - val_loss: 1.3906 - val_acc: 0.7727\n",
            "Epoch 20/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.1551 - acc: 0.9504 - val_loss: 0.2477 - val_acc: 0.9242\n",
            "163/163 [==============================] - 0s 101us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "17:34:27 WORKER: done with job (13, 0, 0), trying to register it.\n",
            "17:34:27 WORKER: registered result for job (13, 0, 0) with dispatcher\n",
            "17:34:27 DISPATCHER: job (13, 0, 0) finished\n",
            "17:34:27 DISPATCHER: register_result: lock acquired\n",
            "17:34:27 DISPATCHER: job (13, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368 finished\n",
            "17:34:27 job_id: (13, 0, 0)\n",
            "kwargs: {'config': {'activation': 'relu', 'batch_size': 3, 'dense_units': 114, 'lr': 0.00019062182011928854, 'number_of_layers': 4, 'optimizer': 'adam'}, 'budget': 20.0, 'working_directory': '.'}\n",
            "result: {'loss': 0.247691522366784, 'info': {'dataset_used': 'Proteomes', 'test_accuracy': [0.33450665168087085, 0.901840490797546]}}\n",
            "exception: None\n",
            "\n",
            "17:34:27 job_callback for (13, 0, 0) started\n",
            "17:34:27 DISPATCHER: Trying to submit another job.\n",
            "17:34:27 job_callback for (13, 0, 0) got condition\n",
            "17:34:27 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
            "17:34:27 HBMASTER: Trying to run another job!\n",
            "17:34:27 job_callback for (13, 0, 0) finished\n",
            "17:34:27 start sampling a new configuration.\n",
            "17:34:27 done sampling a new configuration.\n",
            "17:34:27 HBMASTER: schedule new run for iteration 14\n",
            "17:34:27 HBMASTER: trying submitting job (14, 0, 0) to dispatcher\n",
            "17:34:27 HBMASTER: submitting job (14, 0, 0) to dispatcher\n",
            "17:34:27 DISPATCHER: trying to submit job (14, 0, 0)\n",
            "17:34:27 DISPATCHER: trying to notify the job_runner thread.\n",
            "17:34:27 HBMASTER: job (14, 0, 0) submitted to dispatcher\n",
            "17:34:27 DISPATCHER: Trying to submit another job.\n",
            "17:34:27 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
            "17:34:27 DISPATCHER: starting job (14, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:34:27 DISPATCHER: job (14, 0, 0) dispatched on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:34:27 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
            "17:34:27 WORKER: start processing job (14, 0, 0)\n",
            "17:34:27 WORKER: args: ()\n",
            "17:34:27 WORKER: kwargs: {'config': {'activation': 'tanh', 'batch_size': 3, 'dense_units': 127, 'lr': 0.0006462647167267841, 'number_of_layers': 0, 'optimizer': 'Nadam'}, 'budget': 20.0, 'working_directory': '.'}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 585 samples, validate on 66 samples\n",
            "Epoch 1/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.7915 - acc: 0.6838 - val_loss: 0.6199 - val_acc: 0.6970\n",
            "Epoch 2/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.4047 - acc: 0.8650 - val_loss: 0.5020 - val_acc: 0.7576\n",
            "Epoch 3/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.3007 - acc: 0.8991 - val_loss: 0.3115 - val_acc: 0.8788\n",
            "Epoch 4/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.2459 - acc: 0.9179 - val_loss: 0.2948 - val_acc: 0.8939\n",
            "Epoch 5/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.2110 - acc: 0.9299 - val_loss: 0.2649 - val_acc: 0.9091\n",
            "Epoch 6/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.1779 - acc: 0.9453 - val_loss: 0.2054 - val_acc: 0.9242\n",
            "Epoch 7/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.1612 - acc: 0.9453 - val_loss: 0.1936 - val_acc: 0.9394\n",
            "Epoch 8/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.1463 - acc: 0.9504 - val_loss: 0.2381 - val_acc: 0.9394\n",
            "Epoch 9/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.1275 - acc: 0.9556 - val_loss: 0.2789 - val_acc: 0.9091\n",
            "Epoch 10/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.1133 - acc: 0.9675 - val_loss: 0.2460 - val_acc: 0.9091\n",
            "Epoch 11/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.1003 - acc: 0.9726 - val_loss: 0.2200 - val_acc: 0.9242\n",
            "Epoch 12/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.0984 - acc: 0.9761 - val_loss: 0.2140 - val_acc: 0.9091\n",
            "Epoch 13/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.0871 - acc: 0.9709 - val_loss: 0.1704 - val_acc: 0.9394\n",
            "Epoch 14/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.0692 - acc: 0.9880 - val_loss: 0.2378 - val_acc: 0.8939\n",
            "Epoch 15/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.0668 - acc: 0.9761 - val_loss: 0.3026 - val_acc: 0.8636\n",
            "Epoch 16/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.0606 - acc: 0.9880 - val_loss: 0.2706 - val_acc: 0.8788\n",
            "Epoch 17/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.0601 - acc: 0.9829 - val_loss: 0.1524 - val_acc: 0.9394\n",
            "Epoch 18/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.0495 - acc: 0.9915 - val_loss: 0.5245 - val_acc: 0.8182\n",
            "Epoch 19/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.0491 - acc: 0.9915 - val_loss: 0.2109 - val_acc: 0.8788\n",
            "Epoch 20/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.0484 - acc: 0.9880 - val_loss: 0.2490 - val_acc: 0.9091\n",
            "163/163 [==============================] - 0s 104us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "17:34:46 WORKER: done with job (14, 0, 0), trying to register it.\n",
            "17:34:46 WORKER: registered result for job (14, 0, 0) with dispatcher\n",
            "17:34:46 DISPATCHER: job (14, 0, 0) finished\n",
            "17:34:46 DISPATCHER: register_result: lock acquired\n",
            "17:34:46 DISPATCHER: job (14, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368 finished\n",
            "17:34:46 job_id: (14, 0, 0)\n",
            "kwargs: {'config': {'activation': 'tanh', 'batch_size': 3, 'dense_units': 127, 'lr': 0.0006462647167267841, 'number_of_layers': 0, 'optimizer': 'Nadam'}, 'budget': 20.0, 'working_directory': '.'}\n",
            "result: {'loss': 0.24898229773944794, 'info': {'dataset_used': 'Proteomes', 'test_accuracy': [0.31712682457480784, 0.8773006134969326]}}\n",
            "exception: None\n",
            "\n",
            "17:34:46 job_callback for (14, 0, 0) started\n",
            "17:34:46 DISPATCHER: Trying to submit another job.\n",
            "17:34:46 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
            "17:34:46 job_callback for (14, 0, 0) got condition\n",
            "17:34:46 HBMASTER: Trying to run another job!\n",
            "17:34:46 job_callback for (14, 0, 0) finished\n",
            "17:34:46 start sampling a new configuration.\n",
            "17:34:46 done sampling a new configuration.\n",
            "17:34:46 HBMASTER: schedule new run for iteration 15\n",
            "17:34:46 HBMASTER: trying submitting job (15, 0, 0) to dispatcher\n",
            "17:34:46 HBMASTER: submitting job (15, 0, 0) to dispatcher\n",
            "17:34:46 DISPATCHER: trying to submit job (15, 0, 0)\n",
            "17:34:46 DISPATCHER: trying to notify the job_runner thread.\n",
            "17:34:46 HBMASTER: job (15, 0, 0) submitted to dispatcher\n",
            "17:34:46 DISPATCHER: Trying to submit another job.\n",
            "17:34:46 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
            "17:34:46 DISPATCHER: starting job (15, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:34:46 DISPATCHER: job (15, 0, 0) dispatched on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:34:46 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
            "17:34:46 WORKER: start processing job (15, 0, 0)\n",
            "17:34:46 WORKER: args: ()\n",
            "17:34:46 WORKER: kwargs: {'config': {'activation': 'relu', 'batch_size': 4, 'dense_units': 78, 'lr': 1.1972369887934813e-06, 'number_of_layers': 0, 'optimizer': 'Nadam'}, 'budget': 20.0, 'working_directory': '.'}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 585 samples, validate on 66 samples\n",
            "Epoch 1/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 3.9293 - acc: 0.2171 - val_loss: 3.4980 - val_acc: 0.2576\n",
            "Epoch 2/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 3.7839 - acc: 0.2171 - val_loss: 3.3680 - val_acc: 0.2576\n",
            "Epoch 3/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 3.6471 - acc: 0.2188 - val_loss: 3.2438 - val_acc: 0.2576\n",
            "Epoch 4/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 3.5157 - acc: 0.2239 - val_loss: 3.1257 - val_acc: 0.2576\n",
            "Epoch 5/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 3.3927 - acc: 0.2274 - val_loss: 3.0155 - val_acc: 0.2576\n",
            "Epoch 6/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 3.2778 - acc: 0.2325 - val_loss: 2.9128 - val_acc: 0.2576\n",
            "Epoch 7/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 3.1695 - acc: 0.2393 - val_loss: 2.8160 - val_acc: 0.2576\n",
            "Epoch 8/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 3.0673 - acc: 0.2496 - val_loss: 2.7256 - val_acc: 0.2879\n",
            "Epoch 9/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 2.9723 - acc: 0.2564 - val_loss: 2.6417 - val_acc: 0.2879\n",
            "Epoch 10/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 2.8829 - acc: 0.2667 - val_loss: 2.5640 - val_acc: 0.3182\n",
            "Epoch 11/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 2.7992 - acc: 0.2974 - val_loss: 2.4912 - val_acc: 0.3182\n",
            "Epoch 12/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 2.7211 - acc: 0.3026 - val_loss: 2.4239 - val_acc: 0.3182\n",
            "Epoch 13/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 2.6476 - acc: 0.3179 - val_loss: 2.3608 - val_acc: 0.3485\n",
            "Epoch 14/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 2.5785 - acc: 0.3299 - val_loss: 2.3019 - val_acc: 0.3636\n",
            "Epoch 15/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 2.5131 - acc: 0.3368 - val_loss: 2.2472 - val_acc: 0.3939\n",
            "Epoch 16/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 2.4511 - acc: 0.3282 - val_loss: 2.1956 - val_acc: 0.3939\n",
            "Epoch 17/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 2.3927 - acc: 0.3333 - val_loss: 2.1469 - val_acc: 0.3636\n",
            "Epoch 18/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 2.3368 - acc: 0.3333 - val_loss: 2.1006 - val_acc: 0.3939\n",
            "Epoch 19/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 2.2839 - acc: 0.3350 - val_loss: 2.0568 - val_acc: 0.4091\n",
            "Epoch 20/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 2.2329 - acc: 0.3385 - val_loss: 2.0142 - val_acc: 0.4242\n",
            "163/163 [==============================] - 0s 91us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "17:35:01 WORKER: done with job (15, 0, 0), trying to register it.\n",
            "17:35:01 WORKER: registered result for job (15, 0, 0) with dispatcher\n",
            "17:35:01 DISPATCHER: job (15, 0, 0) finished\n",
            "17:35:01 DISPATCHER: register_result: lock acquired\n",
            "17:35:01 DISPATCHER: job (15, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368 finished\n",
            "17:35:01 job_id: (15, 0, 0)\n",
            "kwargs: {'config': {'activation': 'relu', 'batch_size': 4, 'dense_units': 78, 'lr': 1.1972369887934813e-06, 'number_of_layers': 0, 'optimizer': 'Nadam'}, 'budget': 20.0, 'working_directory': '.'}\n",
            "result: {'loss': 2.01423920284618, 'info': {'dataset_used': 'Proteomes', 'test_accuracy': [2.185045509981963, 0.34969325171657867]}}\n",
            "exception: None\n",
            "\n",
            "17:35:01 job_callback for (15, 0, 0) started\n",
            "17:35:01 DISPATCHER: Trying to submit another job.\n",
            "17:35:01 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
            "17:35:01 job_callback for (15, 0, 0) got condition\n",
            "17:35:01 done building a new model for budget 20.000000 based on 8/13 split\n",
            "Best loss for this budget:0.167374\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "17:35:01 HBMASTER: Trying to run another job!\n",
            "17:35:01 job_callback for (15, 0, 0) finished\n",
            "17:35:01 start sampling a new configuration.\n",
            "17:35:01 best_vector: [1, 0.12471766094830739, 0.8289048120671685, 0.47037474197317564, 0.841876672023026, 1, 0.012566621888869367], 0.01457342110750545, 0.05175506012208522, 0.0007542482856034104\n",
            "17:35:01 done sampling a new configuration.\n",
            "17:35:01 HBMASTER: schedule new run for iteration 16\n",
            "17:35:01 HBMASTER: trying submitting job (16, 0, 0) to dispatcher\n",
            "17:35:01 HBMASTER: submitting job (16, 0, 0) to dispatcher\n",
            "17:35:01 DISPATCHER: trying to submit job (16, 0, 0)\n",
            "17:35:01 DISPATCHER: trying to notify the job_runner thread.\n",
            "17:35:01 HBMASTER: job (16, 0, 0) submitted to dispatcher\n",
            "17:35:01 DISPATCHER: Trying to submit another job.\n",
            "17:35:01 DISPATCHER: starting job (16, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:35:01 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
            "17:35:01 DISPATCHER: job (16, 0, 0) dispatched on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:35:01 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
            "17:35:01 WORKER: start processing job (16, 0, 0)\n",
            "17:35:01 WORKER: args: ()\n",
            "17:35:01 WORKER: kwargs: {'config': {'activation': 'tanh', 'batch_size': 2, 'dense_units': 202, 'lr': 0.0002248400689826989, 'number_of_layers': 4, 'optimizer': 'SGD', 'sgd_momentum': 0.012440955669980673}, 'budget': 20.0, 'working_directory': '.'}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 585 samples, validate on 66 samples\n",
            "Epoch 1/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 1.3563 - acc: 0.4205 - val_loss: 1.2985 - val_acc: 0.4242\n",
            "Epoch 2/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.1679 - acc: 0.5607 - val_loss: 1.1928 - val_acc: 0.4545\n",
            "Epoch 3/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.0582 - acc: 0.6120 - val_loss: 1.0717 - val_acc: 0.6061\n",
            "Epoch 4/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.9649 - acc: 0.6701 - val_loss: 0.9767 - val_acc: 0.6515\n",
            "Epoch 5/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.8860 - acc: 0.7026 - val_loss: 0.9102 - val_acc: 0.6212\n",
            "Epoch 6/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.8156 - acc: 0.7350 - val_loss: 0.8547 - val_acc: 0.5909\n",
            "Epoch 7/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.7547 - acc: 0.7470 - val_loss: 0.8038 - val_acc: 0.6364\n",
            "Epoch 8/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.7021 - acc: 0.7624 - val_loss: 0.7277 - val_acc: 0.7273\n",
            "Epoch 9/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.6567 - acc: 0.7744 - val_loss: 0.6925 - val_acc: 0.7121\n",
            "Epoch 10/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.6148 - acc: 0.7915 - val_loss: 0.7118 - val_acc: 0.6667\n",
            "Epoch 11/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.5850 - acc: 0.7966 - val_loss: 0.6495 - val_acc: 0.7121\n",
            "Epoch 12/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.5469 - acc: 0.8068 - val_loss: 0.6137 - val_acc: 0.7273\n",
            "Epoch 13/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.5249 - acc: 0.8154 - val_loss: 0.5815 - val_acc: 0.7273\n",
            "Epoch 14/20\n",
            "544/585 [==========================>...] - ETA: 0s - loss: 0.5031 - acc: 0.8346"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "17:35:21 DISPATCHER: Starting worker discovery\n",
            "17:35:21 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
            "17:35:21 DISPATCHER: Finished worker discovery\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "585/585 [==============================] - 1s 2ms/step - loss: 0.4960 - acc: 0.8393 - val_loss: 0.5846 - val_acc: 0.7424\n",
            "Epoch 15/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.4753 - acc: 0.8410 - val_loss: 0.5388 - val_acc: 0.7879\n",
            "Epoch 16/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.4522 - acc: 0.8410 - val_loss: 0.5090 - val_acc: 0.7879\n",
            "Epoch 17/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.4345 - acc: 0.8444 - val_loss: 0.5219 - val_acc: 0.8333\n",
            "Epoch 18/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.4178 - acc: 0.8701 - val_loss: 0.4820 - val_acc: 0.8182\n",
            "Epoch 19/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.4038 - acc: 0.8632 - val_loss: 0.4861 - val_acc: 0.8182\n",
            "Epoch 20/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.3763 - acc: 0.8701 - val_loss: 0.4601 - val_acc: 0.8333\n",
            "163/163 [==============================] - 0s 106us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "17:35:30 WORKER: done with job (16, 0, 0), trying to register it.\n",
            "17:35:30 WORKER: registered result for job (16, 0, 0) with dispatcher\n",
            "17:35:30 DISPATCHER: job (16, 0, 0) finished\n",
            "17:35:30 DISPATCHER: register_result: lock acquired\n",
            "17:35:30 DISPATCHER: job (16, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368 finished\n",
            "17:35:30 job_id: (16, 0, 0)\n",
            "kwargs: {'config': {'activation': 'tanh', 'batch_size': 2, 'dense_units': 202, 'lr': 0.0002248400689826989, 'number_of_layers': 4, 'optimizer': 'SGD', 'sgd_momentum': 0.012440955669980673}, 'budget': 20.0, 'working_directory': '.'}\n",
            "result: {'loss': 0.4600763686678626, 'info': {'dataset_used': 'Proteomes', 'test_accuracy': [0.46189846125848455, 0.8098159509202454]}}\n",
            "exception: None\n",
            "\n",
            "17:35:30 job_callback for (16, 0, 0) started\n",
            "17:35:30 DISPATCHER: Trying to submit another job.\n",
            "17:35:30 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
            "17:35:30 job_callback for (16, 0, 0) got condition\n",
            "17:35:30 done building a new model for budget 20.000000 based on 8/14 split\n",
            "Best loss for this budget:0.167374\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "17:35:30 HBMASTER: Trying to run another job!\n",
            "17:35:30 job_callback for (16, 0, 0) finished\n",
            "17:35:30 start sampling a new configuration.\n",
            "17:35:30 best_vector: [0, 0.05863478163449807, 0.5230776835903732, 0.3919215440682092, 0.6948601158691814, 2, 0.07410556595558687], 0.002274396271119077, 1.019322887620128, 0.0023183441746695488\n",
            "17:35:30 done sampling a new configuration.\n",
            "17:35:30 HBMASTER: schedule new run for iteration 17\n",
            "17:35:30 HBMASTER: trying submitting job (17, 0, 0) to dispatcher\n",
            "17:35:30 HBMASTER: submitting job (17, 0, 0) to dispatcher\n",
            "17:35:30 DISPATCHER: trying to submit job (17, 0, 0)\n",
            "17:35:30 DISPATCHER: trying to notify the job_runner thread.\n",
            "17:35:30 HBMASTER: job (17, 0, 0) submitted to dispatcher\n",
            "17:35:30 DISPATCHER: Trying to submit another job.\n",
            "17:35:30 DISPATCHER: starting job (17, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:35:30 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
            "17:35:30 DISPATCHER: job (17, 0, 0) dispatched on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:35:30 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
            "17:35:30 WORKER: start processing job (17, 0, 0)\n",
            "17:35:30 WORKER: args: ()\n",
            "17:35:30 WORKER: kwargs: {'config': {'activation': 'relu', 'batch_size': 2, 'dense_units': 132, 'lr': 9.111874308430073e-05, 'number_of_layers': 3, 'optimizer': 'adam'}, 'budget': 20.0, 'working_directory': '.'}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 585 samples, validate on 66 samples\n",
            "Epoch 1/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 1.1257 - acc: 0.5556 - val_loss: 0.9582 - val_acc: 0.5909\n",
            "Epoch 2/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 0.6863 - acc: 0.7538 - val_loss: 0.6252 - val_acc: 0.7273\n",
            "Epoch 3/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 0.4714 - acc: 0.8205 - val_loss: 0.5574 - val_acc: 0.7424\n",
            "Epoch 4/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 0.3554 - acc: 0.8821 - val_loss: 0.4692 - val_acc: 0.7727\n",
            "Epoch 5/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 0.2969 - acc: 0.8940 - val_loss: 0.4779 - val_acc: 0.7879\n",
            "Epoch 6/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 0.2548 - acc: 0.9197 - val_loss: 0.3818 - val_acc: 0.8485\n",
            "Epoch 7/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 0.2337 - acc: 0.9128 - val_loss: 0.2684 - val_acc: 0.8939\n",
            "Epoch 8/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 0.2103 - acc: 0.9162 - val_loss: 0.2292 - val_acc: 0.8939\n",
            "Epoch 9/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 0.1948 - acc: 0.9128 - val_loss: 0.2425 - val_acc: 0.9242\n",
            "Epoch 10/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 0.1749 - acc: 0.9402 - val_loss: 0.2636 - val_acc: 0.9242\n",
            "Epoch 11/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 0.1638 - acc: 0.9385 - val_loss: 0.2279 - val_acc: 0.9394\n",
            "Epoch 12/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 0.1508 - acc: 0.9487 - val_loss: 0.2217 - val_acc: 0.9091\n",
            "Epoch 13/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 0.1294 - acc: 0.9521 - val_loss: 0.2540 - val_acc: 0.8333\n",
            "Epoch 14/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 0.1352 - acc: 0.9487 - val_loss: 0.3147 - val_acc: 0.8485\n",
            "Epoch 15/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 0.1061 - acc: 0.9692 - val_loss: 0.1673 - val_acc: 0.9394\n",
            "Epoch 16/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 0.1149 - acc: 0.9556 - val_loss: 0.2533 - val_acc: 0.8939\n",
            "Epoch 17/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 0.1019 - acc: 0.9692 - val_loss: 0.1892 - val_acc: 0.9242\n",
            "Epoch 18/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 0.0912 - acc: 0.9726 - val_loss: 0.1423 - val_acc: 0.9242\n",
            "Epoch 19/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 0.0883 - acc: 0.9658 - val_loss: 0.1576 - val_acc: 0.8939\n",
            "Epoch 20/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 0.1136 - acc: 0.9590 - val_loss: 0.1239 - val_acc: 0.9697\n",
            "163/163 [==============================] - 0s 119us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "17:36:05 WORKER: done with job (17, 0, 0), trying to register it.\n",
            "17:36:05 WORKER: registered result for job (17, 0, 0) with dispatcher\n",
            "17:36:05 DISPATCHER: job (17, 0, 0) finished\n",
            "17:36:05 DISPATCHER: register_result: lock acquired\n",
            "17:36:05 DISPATCHER: job (17, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368 finished\n",
            "17:36:05 job_id: (17, 0, 0)\n",
            "kwargs: {'config': {'activation': 'relu', 'batch_size': 2, 'dense_units': 132, 'lr': 9.111874308430073e-05, 'number_of_layers': 3, 'optimizer': 'adam'}, 'budget': 20.0, 'working_directory': '.'}\n",
            "result: {'loss': 0.12385970767916826, 'info': {'dataset_used': 'Proteomes', 'test_accuracy': [0.4015095918182215, 0.8711656441717791]}}\n",
            "exception: None\n",
            "\n",
            "17:36:05 job_callback for (17, 0, 0) started\n",
            "17:36:05 DISPATCHER: Trying to submit another job.\n",
            "17:36:05 job_callback for (17, 0, 0) got condition\n",
            "17:36:05 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
            "17:36:05 done building a new model for budget 20.000000 based on 8/15 split\n",
            "Best loss for this budget:0.123860\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "17:36:05 HBMASTER: Trying to run another job!\n",
            "17:36:05 job_callback for (17, 0, 0) finished\n",
            "17:36:05 start sampling a new configuration.\n",
            "17:36:05 best_vector: [0, 0.5122292899234078, 0.8634234637509568, 0.43987233926837654, 0.06310445680552464, 1, 0.012557725495404022], 3.796577273125009e-05, 80.47024907360083, 0.0030551151879554166\n",
            "17:36:05 done sampling a new configuration.\n",
            "17:36:05 HBMASTER: schedule new run for iteration 18\n",
            "17:36:05 HBMASTER: trying submitting job (18, 0, 0) to dispatcher\n",
            "17:36:05 HBMASTER: submitting job (18, 0, 0) to dispatcher\n",
            "17:36:05 DISPATCHER: trying to submit job (18, 0, 0)\n",
            "17:36:05 DISPATCHER: trying to notify the job_runner thread.\n",
            "17:36:05 HBMASTER: job (18, 0, 0) submitted to dispatcher\n",
            "17:36:05 DISPATCHER: Trying to submit another job.\n",
            "17:36:05 DISPATCHER: starting job (18, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:36:05 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
            "17:36:05 DISPATCHER: job (18, 0, 0) dispatched on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:36:05 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
            "17:36:05 WORKER: start processing job (18, 0, 0)\n",
            "17:36:05 WORKER: args: ()\n",
            "17:36:05 WORKER: kwargs: {'config': {'activation': 'relu', 'batch_size': 4, 'dense_units': 212, 'lr': 0.00015825655090614705, 'number_of_layers': 0, 'optimizer': 'SGD', 'sgd_momentum': 0.012432148240449982}, 'budget': 20.0, 'working_directory': '.'}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 585 samples, validate on 66 samples\n",
            "Epoch 1/20\n",
            "585/585 [==============================] - 1s 1ms/step - loss: 1.4482 - acc: 0.4274 - val_loss: 1.6248 - val_acc: 0.3333\n",
            "Epoch 2/20\n",
            "585/585 [==============================] - 1s 901us/step - loss: 1.1851 - acc: 0.5573 - val_loss: 1.3494 - val_acc: 0.5000\n",
            "Epoch 3/20\n",
            "585/585 [==============================] - 1s 927us/step - loss: 1.0663 - acc: 0.6085 - val_loss: 1.2085 - val_acc: 0.5303\n",
            "Epoch 4/20\n",
            "585/585 [==============================] - 1s 909us/step - loss: 0.9683 - acc: 0.6581 - val_loss: 1.1217 - val_acc: 0.5455\n",
            "Epoch 5/20\n",
            "585/585 [==============================] - 1s 931us/step - loss: 0.8998 - acc: 0.6615 - val_loss: 1.0420 - val_acc: 0.6667\n",
            "Epoch 6/20\n",
            "585/585 [==============================] - 1s 909us/step - loss: 0.8464 - acc: 0.6957 - val_loss: 0.9911 - val_acc: 0.6364\n",
            "Epoch 7/20\n",
            "585/585 [==============================] - 1s 903us/step - loss: 0.7981 - acc: 0.7248 - val_loss: 0.9546 - val_acc: 0.6212\n",
            "Epoch 8/20\n",
            "585/585 [==============================] - 1s 910us/step - loss: 0.7561 - acc: 0.7265 - val_loss: 0.8521 - val_acc: 0.6970\n",
            "Epoch 9/20\n",
            "585/585 [==============================] - 1s 883us/step - loss: 0.7183 - acc: 0.7590 - val_loss: 0.8287 - val_acc: 0.6667\n",
            "Epoch 10/20\n",
            "585/585 [==============================] - 1s 910us/step - loss: 0.6836 - acc: 0.7795 - val_loss: 0.9114 - val_acc: 0.5758\n",
            "Epoch 11/20\n",
            "585/585 [==============================] - 1s 901us/step - loss: 0.6645 - acc: 0.7709 - val_loss: 0.8395 - val_acc: 0.6667\n",
            "Epoch 12/20\n",
            "585/585 [==============================] - 1s 927us/step - loss: 0.6329 - acc: 0.7812 - val_loss: 0.7664 - val_acc: 0.6818\n",
            "Epoch 13/20\n",
            "585/585 [==============================] - 1s 891us/step - loss: 0.6068 - acc: 0.7880 - val_loss: 0.6911 - val_acc: 0.7879\n",
            "Epoch 14/20\n",
            "585/585 [==============================] - 1s 932us/step - loss: 0.5889 - acc: 0.8188 - val_loss: 0.7388 - val_acc: 0.6515\n",
            "Epoch 15/20\n",
            "585/585 [==============================] - 1s 903us/step - loss: 0.5698 - acc: 0.8103 - val_loss: 0.6765 - val_acc: 0.7424\n",
            "Epoch 16/20\n",
            "585/585 [==============================] - 1s 961us/step - loss: 0.5516 - acc: 0.8017 - val_loss: 0.6435 - val_acc: 0.7727\n",
            "Epoch 17/20\n",
            "585/585 [==============================] - 1s 934us/step - loss: 0.5361 - acc: 0.8291 - val_loss: 0.6351 - val_acc: 0.8030\n",
            "Epoch 18/20\n",
            "585/585 [==============================] - 1s 969us/step - loss: 0.5239 - acc: 0.8205 - val_loss: 0.6364 - val_acc: 0.8182\n",
            "Epoch 19/20\n",
            "585/585 [==============================] - 1s 904us/step - loss: 0.5083 - acc: 0.8427 - val_loss: 0.6066 - val_acc: 0.8182\n",
            "Epoch 20/20\n",
            "585/585 [==============================] - 1s 919us/step - loss: 0.4952 - acc: 0.8496 - val_loss: 0.5648 - val_acc: 0.8333\n",
            "163/163 [==============================] - 0s 93us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "17:36:17 WORKER: done with job (18, 0, 0), trying to register it.\n",
            "17:36:17 WORKER: registered result for job (18, 0, 0) with dispatcher\n",
            "17:36:17 DISPATCHER: job (18, 0, 0) finished\n",
            "17:36:17 DISPATCHER: register_result: lock acquired\n",
            "17:36:17 DISPATCHER: job (18, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368 finished\n",
            "17:36:17 job_id: (18, 0, 0)\n",
            "kwargs: {'config': {'activation': 'relu', 'batch_size': 4, 'dense_units': 212, 'lr': 0.00015825655090614705, 'number_of_layers': 0, 'optimizer': 'SGD', 'sgd_momentum': 0.012432148240449982}, 'budget': 20.0, 'working_directory': '.'}\n",
            "result: {'loss': 0.5648174918059147, 'info': {'dataset_used': 'Proteomes', 'test_accuracy': [0.5555837626106168, 0.803680981595092]}}\n",
            "exception: None\n",
            "\n",
            "17:36:17 job_callback for (18, 0, 0) started\n",
            "17:36:17 DISPATCHER: Trying to submit another job.\n",
            "17:36:17 job_callback for (18, 0, 0) got condition\n",
            "17:36:17 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
            "17:36:17 done building a new model for budget 20.000000 based on 8/16 split\n",
            "Best loss for this budget:0.123860\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "17:36:17 HBMASTER: Trying to run another job!\n",
            "17:36:17 job_callback for (18, 0, 0) finished\n",
            "17:36:17 start sampling a new configuration.\n",
            "17:36:17 best_vector: [1, 0.13128731545544187, 0.11348690763907054, 0.31944597995672175, 0.4173595686084875, 1, 0.012837506943723057], 2.639160711841253e-05, 287.1724325147653, 0.007578942014168521\n",
            "17:36:17 done sampling a new configuration.\n",
            "17:36:17 HBMASTER: schedule new run for iteration 19\n",
            "17:36:17 HBMASTER: trying submitting job (19, 0, 0) to dispatcher\n",
            "17:36:17 HBMASTER: submitting job (19, 0, 0) to dispatcher\n",
            "17:36:17 DISPATCHER: trying to submit job (19, 0, 0)\n",
            "17:36:17 DISPATCHER: trying to notify the job_runner thread.\n",
            "17:36:17 HBMASTER: job (19, 0, 0) submitted to dispatcher\n",
            "17:36:17 DISPATCHER: Trying to submit another job.\n",
            "17:36:17 DISPATCHER: starting job (19, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:36:17 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
            "17:36:17 DISPATCHER: job (19, 0, 0) dispatched on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368\n",
            "17:36:17 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
            "17:36:17 WORKER: start processing job (19, 0, 0)\n",
            "17:36:17 WORKER: args: ()\n",
            "17:36:17 WORKER: kwargs: {'config': {'activation': 'tanh', 'batch_size': 2, 'dense_units': 74, 'lr': 3.955759682560897e-05, 'number_of_layers': 2, 'optimizer': 'SGD', 'sgd_momentum': 0.012709131874285826}, 'budget': 20.0, 'working_directory': '.'}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 585 samples, validate on 66 samples\n",
            "Epoch 1/20\n",
            "585/585 [==============================] - 2s 3ms/step - loss: 1.5340 - acc: 0.2547 - val_loss: 1.4749 - val_acc: 0.3182\n",
            "Epoch 2/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.3804 - acc: 0.3590 - val_loss: 1.4243 - val_acc: 0.3485\n",
            "Epoch 3/20\n",
            "446/585 [=====================>........] - ETA: 0s - loss: 1.3092 - acc: 0.4596"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "17:36:21 DISPATCHER: Starting worker discovery\n",
            "17:36:21 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r468/585 [=======================>......] - ETA: 0s - loss: 1.3086 - acc: 0.4594"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "17:36:21 DISPATCHER: Finished worker discovery\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "585/585 [==============================] - 1s 2ms/step - loss: 1.3170 - acc: 0.4615 - val_loss: 1.4012 - val_acc: 0.3939\n",
            "Epoch 4/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.2812 - acc: 0.4803 - val_loss: 1.3838 - val_acc: 0.3788\n",
            "Epoch 5/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.2563 - acc: 0.4838 - val_loss: 1.3667 - val_acc: 0.4091\n",
            "Epoch 6/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.2360 - acc: 0.5009 - val_loss: 1.3517 - val_acc: 0.4091\n",
            "Epoch 7/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.2182 - acc: 0.5145 - val_loss: 1.3395 - val_acc: 0.4394\n",
            "Epoch 8/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.2018 - acc: 0.5265 - val_loss: 1.3215 - val_acc: 0.4242\n",
            "Epoch 9/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.1873 - acc: 0.5368 - val_loss: 1.3080 - val_acc: 0.4394\n",
            "Epoch 10/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.1727 - acc: 0.5453 - val_loss: 1.2940 - val_acc: 0.4697\n",
            "Epoch 11/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.1591 - acc: 0.5590 - val_loss: 1.2806 - val_acc: 0.4697\n",
            "Epoch 12/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.1436 - acc: 0.5880 - val_loss: 1.2724 - val_acc: 0.4848\n",
            "Epoch 13/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.1322 - acc: 0.5897 - val_loss: 1.2552 - val_acc: 0.4848\n",
            "Epoch 14/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.1192 - acc: 0.6017 - val_loss: 1.2488 - val_acc: 0.4697\n",
            "Epoch 15/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.1071 - acc: 0.5897 - val_loss: 1.2340 - val_acc: 0.4848\n",
            "Epoch 16/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.0967 - acc: 0.6000 - val_loss: 1.2258 - val_acc: 0.4848\n",
            "Epoch 17/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.0858 - acc: 0.6137 - val_loss: 1.2166 - val_acc: 0.5152\n",
            "Epoch 18/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.0756 - acc: 0.6256 - val_loss: 1.2064 - val_acc: 0.5000\n",
            "Epoch 19/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.0658 - acc: 0.6239 - val_loss: 1.1966 - val_acc: 0.5000\n",
            "Epoch 20/20\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.0565 - acc: 0.6239 - val_loss: 1.1870 - val_acc: 0.5152\n",
            "163/163 [==============================] - 0s 96us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "17:36:42 WORKER: done with job (19, 0, 0), trying to register it.\n",
            "17:36:42 WORKER: registered result for job (19, 0, 0) with dispatcher\n",
            "17:36:42 DISPATCHER: job (19, 0, 0) finished\n",
            "17:36:42 DISPATCHER: register_result: lock acquired\n",
            "17:36:42 DISPATCHER: job (19, 0, 0) on hpbandster.run_run_01.worker.01360c10cc69.143140147540842368 finished\n",
            "17:36:42 job_id: (19, 0, 0)\n",
            "kwargs: {'config': {'activation': 'tanh', 'batch_size': 2, 'dense_units': 74, 'lr': 3.955759682560897e-05, 'number_of_layers': 2, 'optimizer': 'SGD', 'sgd_momentum': 0.012709131874285826}, 'budget': 20.0, 'working_directory': '.'}\n",
            "result: {'loss': 1.1870399424524019, 'info': {'dataset_used': 'Proteomes', 'test_accuracy': [1.0247236168457687, 0.6687116566245541]}}\n",
            "exception: None\n",
            "\n",
            "17:36:42 job_callback for (19, 0, 0) started\n",
            "17:36:42 DISPATCHER: Trying to submit another job.\n",
            "17:36:42 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
            "17:36:42 job_callback for (19, 0, 0) got condition\n",
            "17:36:42 done building a new model for budget 20.000000 based on 8/17 split\n",
            "Best loss for this budget:0.123860\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "17:36:42 HBMASTER: Trying to run another job!\n",
            "17:36:42 job_callback for (19, 0, 0) finished\n",
            "17:36:42 HBMASTER: shutdown initiated, shutdown_workers = True\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best run:{'activation': 'relu', 'batch_size': 2, 'dense_units': 132, 'lr': 9.111874308430073e-05, 'number_of_layers': 3, 'optimizer': 'adam'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "17:36:42 WORKER: shutting down now!\n",
            "17:36:43 DISPATCHER: Dispatcher shutting down\n",
            "17:36:43 DISPATCHER: Trying to submit another job.\n",
            "17:36:43 DISPATCHER: job_runner shutting down\n",
            "17:36:43 DISPATCHER: discover_workers shutting down\n",
            "17:36:43 DISPATCHER: 'discover_worker' thread exited\n",
            "17:36:43 DISPATCHER: 'job_runner' thread exited\n",
            "17:36:43 DISPATCHER: shut down complete\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "jkFBrmMH62U5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " def run_model(config, budget):\n",
        "        \"\"\"\n",
        "        Get model with hyperparameters from config generated by get_configspace()\n",
        "        \"\"\"\n",
        "\n",
        "        K.clear_session()  # to reset the network graph for each model\n",
        "\n",
        "        a = datetime.datetime.now()\n",
        "\n",
        "        budget = int(budget)\n",
        "        model = Sequential()\n",
        "        model.add(Dense(int(config['dense_units']), input_dim=64, kernel_initializer='normal', activation=config['activation']))\n",
        "        for i in range(0,config['number_of_layers']):\n",
        "          model.add(Dense(int(config['dense_units']), activation=config['activation']))\n",
        "        model.add(Dense(5, activation='softmax'))\n",
        "        \n",
        "        if config['optimizer'] == 'Nadam':\n",
        "            optimizer = keras.optimizers.Nadam(config['lr'])\n",
        "        elif config['optimizer'] == 'SGD':\n",
        "          \n",
        "            optimizer = keras.optimizers.SGD(config['lr'], config['sgd_momentum'])\n",
        "        elif config['optimizer'] == 'adam':\n",
        "            optimizer = keras.optimizers.adam(config['lr'])\n",
        "        model.compile(loss=keras.losses.categorical_crossentropy, optimizer=optimizer, metrics=['accuracy'])\n",
        "        history = model.fit(X_train,y_train, epochs=budget, batch_size=config['batch_size'],  verbose=1, validation_data=(X_valid, y_valid))\n",
        "        testing_score = model.evaluate(X_test,y_test)\n",
        "        c = datetime.datetime.now() - a\n",
        "        validation_loss = history.history['val_loss'][-1]\n",
        "        validation_accuracy = history.history['val_acc'][-1]\n",
        "        test_score = model.evaluate(x = X_test, y= y_test)\n",
        "\n",
        "        print(f'Validation Loss:{validation_loss}, Validation Accuracy:{validation_accuracy}, Test Accuracy:{test_score[1]} with epochs:{20}')\n",
        "\n",
        "        plt.plot(history.history['loss'])\n",
        "        plt.plot(history.history['val_loss'])\n",
        "        plt.title('BOHB Optimised')\n",
        "        plt.ylabel('loss')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.legend(['train', 'test'], loc='upper left')\n",
        "        #plt.savefig(os.path.joins(directory, 'Training_curve.png'))\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Rtppq9yAVeo",
        "colab_type": "code",
        "outputId": "d2d3dd9d-e3fd-4e35-91f5-fc2675a48334",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1532
        }
      },
      "cell_type": "code",
      "source": [
        "config = {'activation': 'relu', 'batch_size': 3, 'dense_units': 110, 'lr': 5.103817425944183e-05, 'number_of_layers': 1, 'optimizer': 'adam'}\n",
        "df1 = pd.read_csv(\"BC_Data1.csv\")\n",
        "X = pd.DataFrame(data=df1,columns= ['Age', 'Tumor Size','Nodes','KI67','Basal-like Score','Luminal A Score','Luminal B Score', 'HER2-enriched Score',\n",
        "                            'Normal Score','ESR1 Score','ERBB2 Score','PGR Score','Proliferation Score','Luminal Score',\n",
        "                            'ACTR3B','ANLN','BAG1','BCL2','BLVRA','CCNE1','CDC6','CDH3','CXXC5', 'EGFR' ,'ERBB2', 'ESR1','EXO1', 'FGFR4',\n",
        "                            'FOXA1', 'FOXC1', 'GRB7', 'KRT14', 'KRT17','KRT5', 'MAPT','MDM2','MIA','MLPH','MMP11', 'MYBL2', 'MYC', 'NAT1',\n",
        "                            'ORC6L', 'PGR', 'PHGDH', 'SFRP1' , 'SLC39A6', 'UBE2T' ,'CDC20' ,'MKI67', 'RRM2','TYMS', 'UBE2C', 'CENPF', 'GPR160',\n",
        "                            'KIF2C', 'MELK', 'TMEM45B', 'BIRC5', 'CCNB1','CDCA1','CEP55', 'KNTC2','PTTG1'])\n",
        "X['KI67'] = X['KI67'].fillna(0)\n",
        "y = pd.DataFrame(data=df1,columns= ['Subtype Prediction'])\n",
        "X = np.array(X).astype(np.float64)\n",
        "y = np.array(y)\n",
        "one = OneHotEncoder() #initiate class\n",
        "y = one.fit_transform(y)\n",
        "X,y = shuffle(X,y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1)\n",
        "\n",
        "\n",
        "run_model(config,30)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 585 samples, validate on 66 samples\n",
            "Epoch 1/30\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.6543 - acc: 0.4171 - val_loss: 1.0969 - val_acc: 0.6667\n",
            "Epoch 2/30\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 1.0510 - acc: 0.6137 - val_loss: 0.8504 - val_acc: 0.7424\n",
            "Epoch 3/30\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.8660 - acc: 0.7179 - val_loss: 0.7138 - val_acc: 0.8182\n",
            "Epoch 4/30\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.7429 - acc: 0.7573 - val_loss: 0.6515 - val_acc: 0.8182\n",
            "Epoch 5/30\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.6603 - acc: 0.7829 - val_loss: 0.5885 - val_acc: 0.8333\n",
            "Epoch 6/30\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.5843 - acc: 0.8188 - val_loss: 0.5219 - val_acc: 0.8788\n",
            "Epoch 7/30\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.5233 - acc: 0.8496 - val_loss: 0.5046 - val_acc: 0.8939\n",
            "Epoch 8/30\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.4711 - acc: 0.8581 - val_loss: 0.4558 - val_acc: 0.8939\n",
            "Epoch 9/30\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.4311 - acc: 0.8735 - val_loss: 0.4373 - val_acc: 0.8788\n",
            "Epoch 10/30\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.3961 - acc: 0.8872 - val_loss: 0.4276 - val_acc: 0.8636\n",
            "Epoch 11/30\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.3682 - acc: 0.8923 - val_loss: 0.3960 - val_acc: 0.8788\n",
            "Epoch 12/30\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.3459 - acc: 0.9111 - val_loss: 0.3830 - val_acc: 0.8788\n",
            "Epoch 13/30\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.3241 - acc: 0.9145 - val_loss: 0.3811 - val_acc: 0.8636\n",
            "Epoch 14/30\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.3036 - acc: 0.9145 - val_loss: 0.3756 - val_acc: 0.8636\n",
            "Epoch 15/30\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.2962 - acc: 0.9162 - val_loss: 0.3744 - val_acc: 0.8636\n",
            "Epoch 16/30\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.2745 - acc: 0.9128 - val_loss: 0.3571 - val_acc: 0.8636\n",
            "Epoch 17/30\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.2603 - acc: 0.9231 - val_loss: 0.3470 - val_acc: 0.8939\n",
            "Epoch 18/30\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.2485 - acc: 0.9316 - val_loss: 0.3471 - val_acc: 0.8636\n",
            "Epoch 19/30\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.2399 - acc: 0.9402 - val_loss: 0.3585 - val_acc: 0.8636\n",
            "Epoch 20/30\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.2300 - acc: 0.9333 - val_loss: 0.3475 - val_acc: 0.8636\n",
            "Epoch 21/30\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.2166 - acc: 0.9368 - val_loss: 0.3376 - val_acc: 0.8636\n",
            "Epoch 22/30\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.2090 - acc: 0.9470 - val_loss: 0.3412 - val_acc: 0.8636\n",
            "Epoch 23/30\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.2033 - acc: 0.9504 - val_loss: 0.3349 - val_acc: 0.8636\n",
            "Epoch 24/30\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.1976 - acc: 0.9487 - val_loss: 0.3255 - val_acc: 0.8636\n",
            "Epoch 25/30\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.1866 - acc: 0.9538 - val_loss: 0.3201 - val_acc: 0.8788\n",
            "Epoch 26/30\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.1797 - acc: 0.9573 - val_loss: 0.3829 - val_acc: 0.8485\n",
            "Epoch 27/30\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.1804 - acc: 0.9487 - val_loss: 0.3806 - val_acc: 0.8636\n",
            "Epoch 28/30\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.1729 - acc: 0.9453 - val_loss: 0.3264 - val_acc: 0.8636\n",
            "Epoch 29/30\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.1644 - acc: 0.9556 - val_loss: 0.3189 - val_acc: 0.8636\n",
            "Epoch 30/30\n",
            "585/585 [==============================] - 1s 2ms/step - loss: 0.1609 - acc: 0.9607 - val_loss: 0.3473 - val_acc: 0.8636\n",
            "163/163 [==============================] - 0s 106us/step\n",
            "163/163 [==============================] - 0s 105us/step\n",
            "Validation Loss:0.34728865055843594, Validation Accuracy:0.8636363717642698, Test Accuracy:0.8957055218380653 with epochs:20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "17:45:25 update_title_pos\n",
            "17:45:25 update_title_pos\n",
            "17:45:25 update_title_pos\n",
            "17:45:25 update_title_pos\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFnCAYAAACPasF4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbxERkRSj8BYREUkxCm8REZEU838mCXbGkIrG8QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "W9dk5gHjA8MS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}